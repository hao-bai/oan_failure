{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Deep Neural Network\n",
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Optical Dataset composed of\n",
      "46110 source samples\n",
      "50862 source background samples\n",
      "438 target labeled samples\n",
      "8202 target unlabeled samples\n",
      "29592 target background samples\n",
      " Optical Dataset labels composed of\n",
      "46110 labels of source samples\n",
      "438 labels of target samples\n",
      "\n",
      "Test data\n",
      "Optical Dataset composed of\n",
      "0 source samples\n",
      "0 source background samples\n",
      "17758 target labeled samples\n",
      "0 target unlabeled samples\n",
      "47275 target background samples\n",
      " Optical Dataset labels composed of\n",
      "0 labels of source samples\n",
      "17758 labels of target samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from load import load_all_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset()\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46110, 672, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除NaN\n",
    "from feature_extractor import FeatureExtractor\n",
    "from numpy import newaxis\n",
    "# class FE(FeatureExtractor):\n",
    "#     def transform(self, X):\n",
    "#         np.nan_to_num(X, copy=False)\n",
    "#         return X[:,:,:,newaxis]\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "X_train.source = fe.transform(X_train.source)\n",
    "X_train.target = fe.transform(X_train.target)\n",
    "X_test.target = fe.transform(X_test.target)\n",
    "X_train.source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集转换为TensorFlow格式\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.source, y_train.source)).batch(16)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_train.target, y_train.target)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target))\n",
    "\n",
    "# 额外操作\n",
    "train_dataset = train_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) )\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络模型\n",
    "参考资料\n",
    "\n",
    "1. [Build your first Neural Network in TensorFlow 2](https://towardsdatascience.com/building-your-first-neural-network-in-tensorflow-2-tensorflow-for-hackers-part-i-e1e2f1dfe7a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer (Flatten)        (None, 6720)              0         \n",
      "_________________________________________________________________\n",
      "Layer1 (Dense)               (None, 256)               1720576   \n",
      "_________________________________________________________________\n",
      "Layer2 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer3 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer4 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer5 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer6 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer7 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer8 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer9 (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer10 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "Layer-1 (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,312,961\n",
      "Trainable params: 2,312,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "model = tf.keras.Sequential()\n",
    "model.add( layers.Flatten(input_shape=(672, 10,), name=\"Input_Layer\") )\n",
    "\n",
    "num_fully_connected_layers = 10\n",
    "for i in range(num_fully_connected_layers):\n",
    "    model.add( layers.Dense(256, activation=\"relu\", name=\"Layer{}\".format(i+1)) )\n",
    "\n",
    "model.add( layers.Dropout(0.5, name=\"Layer-1\"))\n",
    "model.add( layers.Dense(1, activation='softmax', name=\"Output_Layer\") )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 86144.6875 - acc: 0.1081 - val_loss: 7373.7734 - val_acc: 0.2917\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 105237.9375 - acc: 0.0916 - val_loss: 2291.2295 - val_acc: 0.2917\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 15302.2227 - acc: 0.0797 - val_loss: 38625.6445 - val_acc: 0.2917\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 221495.7656 - acc: 0.0828 - val_loss: 22115.0391 - val_acc: 0.2917\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 55263.8711 - acc: 0.0791 - val_loss: 39988.9688 - val_acc: 0.2917\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 159432.4688 - acc: 0.0744 - val_loss: 70399.4141 - val_acc: 0.2917\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 147974.2031 - acc: 0.0781 - val_loss: 156766.9688 - val_acc: 0.2917\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1064643.0000 - acc: 0.0916 - val_loss: 11510.0625 - val_acc: 0.2917\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 302550.3750 - acc: 0.1063 - val_loss: 2083.5652 - val_acc: 0.2917\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 460227.1875 - acc: 0.1181 - val_loss: 100232.3438 - val_acc: 0.2917\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1120920.5000 - acc: 0.1241 - val_loss: 1369.5067 - val_acc: 0.2917\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 250842.2188 - acc: 0.1322 - val_loss: 287779.7500 - val_acc: 0.2917\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 586183.2500 - acc: 0.1284 - val_loss: 708.5229 - val_acc: 0.2917\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 364477.2812 - acc: 0.1359 - val_loss: 120395.5625 - val_acc: 0.2917\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 251058.7969 - acc: 0.1273 - val_loss: 8018.8481 - val_acc: 0.2917\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2540.1809 - acc: 0.0969 - val_loss: 4417.3833 - val_acc: 0.2917\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 29545.4375 - acc: 0.0825 - val_loss: 12343.4795 - val_acc: 0.2917\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 29270.4727 - acc: 0.0825 - val_loss: 246.1088 - val_acc: 0.2917\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 24620.2480 - acc: 0.0772 - val_loss: 1185.2081 - val_acc: 0.2917\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 36031.6211 - acc: 0.0794 - val_loss: 22795.6660 - val_acc: 0.2917\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 443512.2812 - acc: 0.0759 - val_loss: 4842.8857 - val_acc: 0.2917\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 41686.2617 - acc: 0.0862 - val_loss: 672.4644 - val_acc: 0.2917\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 144193.5156 - acc: 0.0984 - val_loss: 79636.7109 - val_acc: 0.2917\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 111239.1172 - acc: 0.1141 - val_loss: 5262.8760 - val_acc: 0.2917\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 120248.8828 - acc: 0.1206 - val_loss: 11834.0166 - val_acc: 0.2917\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 59039.1367 - acc: 0.1297 - val_loss: 8762.5732 - val_acc: 0.2917\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1519567.3750 - acc: 0.1275 - val_loss: 14620.9502 - val_acc: 0.2917\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 202495.4531 - acc: 0.1359 - val_loss: 1706.3213 - val_acc: 0.2917\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 150849.7188 - acc: 0.1373 - val_loss: 17383.3926 - val_acc: 0.2917\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 43727.6484 - acc: 0.1034 - val_loss: 34.5993 - val_acc: 0.2917\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 83490.2734 - acc: 0.0909 - val_loss: 5045.6372 - val_acc: 0.2917\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 22213.1582 - acc: 0.0778 - val_loss: 11431.3467 - val_acc: 0.2917\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2918.7854 - acc: 0.0838 - val_loss: 8604.9053 - val_acc: 0.2917\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12269.3447 - acc: 0.0766 - val_loss: 9859.5664 - val_acc: 0.2917\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 48385.2695 - acc: 0.0750 - val_loss: 27493.7402 - val_acc: 0.2917\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 132805.2969 - acc: 0.0822 - val_loss: 9796.7529 - val_acc: 0.2917\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 58962.4883 - acc: 0.0934 - val_loss: 2619.0798 - val_acc: 0.2917\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 42385.3789 - acc: 0.1081 - val_loss: 6144.1562 - val_acc: 0.2917\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 115994.0781 - acc: 0.1187 - val_loss: 1181.2452 - val_acc: 0.2917\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 99746.9531 - acc: 0.1256 - val_loss: 5189.6909 - val_acc: 0.2917\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 109908.3594 - acc: 0.1322 - val_loss: 1065.6700 - val_acc: 0.2917\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 22089.4766 - acc: 0.1281 - val_loss: 10402.5459 - val_acc: 0.2917\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 153087.0938 - acc: 0.1428 - val_loss: 45041.0664 - val_acc: 0.2917\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 84501.6953 - acc: 0.1182 - val_loss: 7814.4839 - val_acc: 0.2917\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 53941.5469 - acc: 0.0919 - val_loss: 11361.9854 - val_acc: 0.2917\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 46436.1055 - acc: 0.0816 - val_loss: 8476.1143 - val_acc: 0.2917\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 34514.7266 - acc: 0.0803 - val_loss: 28.3347 - val_acc: 0.2917\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 32489.7656 - acc: 0.0800 - val_loss: 39189.2852 - val_acc: 0.2917\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 185980.6875 - acc: 0.0791 - val_loss: 7984.3594 - val_acc: 0.2917\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 31279.5977 - acc: 0.0750 - val_loss: 1103.9923 - val_acc: 0.2917\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 63371.6289 - acc: 0.0872 - val_loss: 22155.3359 - val_acc: 0.2917\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 20689.4355 - acc: 0.1034 - val_loss: 16256.0518 - val_acc: 0.2917\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 431032.1562 - acc: 0.1163 - val_loss: 8581.9209 - val_acc: 0.2917\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 192464.1406 - acc: 0.1225 - val_loss: 2996.1716 - val_acc: 0.2917\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3239.6604 - acc: 0.1303 - val_loss: 65.2289 - val_acc: 0.2917\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 204082.8750 - acc: 0.1287 - val_loss: 8705.7783 - val_acc: 0.2917\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 201411.7656 - acc: 0.1356 - val_loss: 150.5921 - val_acc: 0.2917\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 345211.0000 - acc: 0.1316 - val_loss: 2266.3630 - val_acc: 0.2917\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 40660.8594 - acc: 0.1031 - val_loss: 777.5308 - val_acc: 0.2917\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12266.5547 - acc: 0.0838 - val_loss: 5333.9336 - val_acc: 0.2917\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 41658.7383 - acc: 0.0825 - val_loss: 13053.5596 - val_acc: 0.2917\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12071.6387 - acc: 0.0809 - val_loss: 1046.8420 - val_acc: 0.2917\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11604.5537 - acc: 0.0775 - val_loss: 2756.1152 - val_acc: 0.2917\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4945.8696 - acc: 0.0725 - val_loss: 4260.9995 - val_acc: 0.2917\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 34213.8281 - acc: 0.0841 - val_loss: 3371.9324 - val_acc: 0.2917\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 22459.9492 - acc: 0.0956 - val_loss: 22491.7402 - val_acc: 0.2917\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5854.5737 - acc: 0.1122 - val_loss: 14743.7734 - val_acc: 0.2917\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 281463.6875 - acc: 0.1184 - val_loss: 23626.0605 - val_acc: 0.2917\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 48801.8359 - acc: 0.1312 - val_loss: 5102.0796 - val_acc: 0.2917\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 72593.3047 - acc: 0.1275 - val_loss: 7571.1704 - val_acc: 0.2917\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6518.3574 - acc: 0.1316 - val_loss: 16338.2930 - val_acc: 0.2917\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 710291.3750 - acc: 0.1412 - val_loss: 83320.1953 - val_acc: 0.2917\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 20166.4336 - acc: 0.1107 - val_loss: 1887.2377 - val_acc: 0.2917\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 58291.6641 - acc: 0.0928 - val_loss: 8635.0986 - val_acc: 0.2917\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 45840.3594 - acc: 0.0806 - val_loss: 8525.2988 - val_acc: 0.2917\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 34121.5586 - acc: 0.0812 - val_loss: 532.0189 - val_acc: 0.2917\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 224506.2656 - acc: 0.0772 - val_loss: 2132.4441 - val_acc: 0.2917\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 734.2850 - acc: 0.0763 - val_loss: 5908.2090 - val_acc: 0.2917\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 34521.4219 - acc: 0.0800 - val_loss: 433.9046 - val_acc: 0.2917\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9448.8008 - acc: 0.0897 - val_loss: 16672.2852 - val_acc: 0.2917\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 176804.2188 - acc: 0.1047 - val_loss: 625.1513 - val_acc: 0.2917\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 27180.5371 - acc: 0.1178 - val_loss: 9404.1914 - val_acc: 0.2917\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 39418.9531 - acc: 0.1247 - val_loss: 2603.6340 - val_acc: 0.2917\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 167385.1250 - acc: 0.1322 - val_loss: 578.9105 - val_acc: 0.2917\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 112034.2891 - acc: 0.1284 - val_loss: 10379.3838 - val_acc: 0.2917\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 52186.1602 - acc: 0.1350 - val_loss: 360.3117 - val_acc: 0.2917\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 31923.4121 - acc: 0.1279 - val_loss: 1147.2198 - val_acc: 0.2917\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2416.7017 - acc: 0.0991 - val_loss: 4008.2402 - val_acc: 0.2917\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11711.3242 - acc: 0.0816 - val_loss: 3074.8223 - val_acc: 0.2917\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9510.0645 - acc: 0.0825 - val_loss: 358.5760 - val_acc: 0.2917\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 7392.3408 - acc: 0.0781 - val_loss: 122.4646 - val_acc: 0.2917\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2187.8938 - acc: 0.0797 - val_loss: 1746.7618 - val_acc: 0.2917\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 497.9489 - acc: 0.0756 - val_loss: 241.3731 - val_acc: 0.2917\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 201.2638 - acc: 0.0856 - val_loss: 34.3064 - val_acc: 0.2917\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.8519 - acc: 0.0981 - val_loss: 14.8410 - val_acc: 0.2917\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9810 - acc: 0.1141 - val_loss: 1.0788 - val_acc: 0.2917\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5480 - acc: 0.1175 - val_loss: 1.1901 - val_acc: 0.2917\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 13.5755 - acc: 0.1306 - val_loss: 1.1060 - val_acc: 0.2917\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6631 - acc: 0.1284 - val_loss: 1.1283 - val_acc: 0.2917\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8089 - acc: 0.1372 - val_loss: 1.0211 - val_acc: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff62c7ada30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  keras.callbacks.TensorBoard(log_dir='./log/{}'.format(\n",
    "      dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")), write_images=True),\n",
    "  ]\n",
    "model.compile(optimizer=\"Adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}