{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集概览\n",
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Optical Dataset composed of\n",
      "46110 source samples\n",
      "50862 source background samples\n",
      "438 target labeled samples\n",
      "8202 target unlabeled samples\n",
      "29592 target background samples\n",
      " Optical Dataset labels composed of\n",
      "46110 labels of source samples\n",
      "438 labels of target samples\n",
      "\n",
      "Test data\n",
      "Optical Dataset composed of\n",
      "0 source samples\n",
      "0 source background samples\n",
      "17758 target labeled samples\n",
      "0 target unlabeled samples\n",
      "47275 target background samples\n",
      " Optical Dataset labels composed of\n",
      "0 labels of source samples\n",
      "17758 labels of target samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from load import load_all_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset()\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集\n",
    "### City A (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source labeled (City A weak & failure): (46110, 672, 10)\n",
      "|- labels: (46110,)\n",
      " |- weak=0: (41341,)\n",
      " |- failure=1: (4769,)\n",
      "source background (City A good): (50862, 672, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"source labeled (City A weak & failure):\", X_train.source.shape)\n",
    "# print(np.where(np.isnan(X_train.source[0])))\n",
    "print(\"|- labels:\", y_train.source.shape)\n",
    "print(\" |- weak=0:\", y_train.source[y_train.source==0].shape)\n",
    "print(\" |- failure=1:\", y_train.source[y_train.source==1].shape)\n",
    "print(\"source background (City A good):\", X_train.source_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target labeled (City B weak & failure): (438, 672, 10, 1)\n",
      "|- labels: (438,)\n",
      " |- weak=0: (349,)\n",
      " |- failure=1: (89,)\n",
      "target background (City B good): (29592, 672, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_train.target.shape)\n",
    "print(\"|- labels:\", y_train.target.shape)\n",
    "print(\" |- weak=0:\", y_train.target[y_train.target==0].shape)\n",
    "print(\" |- failure=1:\", y_train.target[y_train.target==1].shape)\n",
    "print(\"target background (City B good):\", X_train.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集\n",
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target labeled (City B weak & failure): (17758, 672, 10)\n",
      "|- labels: (17758,)\n",
      " |- weak=0: (15464,)\n",
      " |- failure=1: (2294,)\n",
      "target background (City B good): (47275, 672, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_test.target.shape)\n",
    "print(\"|- labels:\", y_test.target.shape)\n",
    "print(\" |- weak=0:\", y_test.target[y_test.target==0].shape)\n",
    "print(\" |- failure=1:\", y_test.target[y_test.target==1].shape)\n",
    "print(\"target background (City B good):\", X_test.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network\n",
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46110, 672, 10, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除NaN\n",
    "from feature_extractor import FeatureExtractor\n",
    "from numpy import newaxis\n",
    "class FE(FeatureExtractor):\n",
    "    def transform(self, X):\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        return X[:,:,:,newaxis]\n",
    "\n",
    "fe = FE()\n",
    "X_train.source = fe.transform(X_train.source)\n",
    "X_train.target = fe.transform(X_train.target)\n",
    "X_test.target = fe.transform(X_test.target)\n",
    "X_train.source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集转换为TensorFlow格式\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.source, y_train.source)).batch(16)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_train.target, y_train.target)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target))\n",
    "\n",
    "# 额外操作\n",
    "train_dataset = train_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) )\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[0.000]\n"
     ]
    }
   ],
   "source": [
    "from numpy import zeros, newaxis\n",
    "a= zeros((4,3))\n",
    "print(a[0][0])\n",
    "b=a[:,:,newaxis]\n",
    "print(b[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络模型\n",
    "参考资料\n",
    "\n",
    "1. [Introduction to ResNet in TensorFlow 2](https://adventuresinmachinelearning.com/introduction-resnet-tensorflow-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(filters, conv_size, input_data):\n",
    "    ''' A residual block of 3 layers\n",
    "    '''\n",
    "    # 1st layer with batch normalization\n",
    "    x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 2nd layer with batch normalization, but no activation function\n",
    "    x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 3rd layer is residual addition with an activation function\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Layer (InputLayer)        [(None, 672, 10, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Layer01 (Conv2D)                (None, 671, 9, 32)   160         Input_Layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Layer02 (Conv2D)                (None, 670, 8, 64)   8256        Layer01[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer03 (MaxPooling2D)          (None, 335, 4, 64)   0           Layer02[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 335, 4, 64)   16448       Layer03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 335, 4, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 335, 4, 64)   16448       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 335, 4, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 335, 4, 64)   0           batch_normalization_1[0][0]      \n",
      "                                                                 Layer03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 335, 4, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 335, 4, 64)   16448       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 335, 4, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 335, 4, 64)   16448       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 335, 4, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 335, 4, 64)   0           batch_normalization_3[0][0]      \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 335, 4, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 335, 4, 64)   16448       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 335, 4, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 335, 4, 64)   16448       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 335, 4, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 335, 4, 64)   0           batch_normalization_5[0][0]      \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 335, 4, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 335, 4, 64)   16448       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 335, 4, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 335, 4, 64)   16448       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 335, 4, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 335, 4, 64)   0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 335, 4, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 335, 4, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 335, 4, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 335, 4, 64)   16448       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 335, 4, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 335, 4, 64)   0           batch_normalization_9[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 335, 4, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 335, 4, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 335, 4, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 335, 4, 64)   16448       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 335, 4, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 335, 4, 64)   0           batch_normalization_11[0][0]     \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 335, 4, 64)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 335, 4, 64)   16448       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 335, 4, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 335, 4, 64)   16448       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 335, 4, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 335, 4, 64)   0           batch_normalization_13[0][0]     \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 335, 4, 64)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 335, 4, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 335, 4, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 335, 4, 64)   16448       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 335, 4, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 335, 4, 64)   0           batch_normalization_15[0][0]     \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 335, 4, 64)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 335, 4, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 335, 4, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 335, 4, 64)   16448       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 335, 4, 64)   256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 335, 4, 64)   0           batch_normalization_17[0][0]     \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 335, 4, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 335, 4, 64)   16448       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 335, 4, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 335, 4, 64)   16448       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 335, 4, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 335, 4, 64)   0           batch_normalization_19[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 335, 4, 64)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Layer-4 (Conv2D)                (None, 334, 3, 64)   16448       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Layer-3 (GlobalAveragePooling2D (None, 64)           0           Layer-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer-2 (Dense)                 (None, 256)          16640       Layer-3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer-1 (Dropout)               (None, 256)          0           Layer-2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output_Layer (Dense)            (None, 1)            257         Layer-1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 375,841\n",
      "Trainable params: 373,281\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Layer to be used as an entry point into a Network (a graph of layers).\n",
    "# https://keras.io/api/layers/core_layers/input/\n",
    "inputs = keras.Input(shape=(672, 10, 1), name=\"Input_Layer\")\n",
    "# inputs = layers.InputLayer(input_shape=(672, 10), name=\"Input_Layer\")\n",
    "\n",
    "# 2D convolution layer (e.g. spatial convolution over images).\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "x = layers.Conv2D(32, 2, activation='relu', name=\"Layer01\")(inputs)\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer02\")(x)\n",
    "\n",
    "# Max pooling operation for 2D spatial data.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "# pool_size: integer or tuple of 2 integers, window size over which to take the maximum.\n",
    "x = layers.MaxPooling2D(2, name=\"Layer03\")(x)\n",
    "\n",
    "num_res_net_blocks = 10 # 10个ResNet blocks\n",
    "for i in range(num_res_net_blocks):\n",
    "    x = res_net_block(64, 2, x)\n",
    "\n",
    "# [Final layers] a standard CNN layer\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer-4\")(x)\n",
    "# [Final layers] GAP layer\n",
    "x = layers.GlobalAveragePooling2D(name=\"Layer-3\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "# Just your regular densely-connected NN layer.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "# units: Positive integer, dimensionality of the output space.\n",
    "x = layers.Dense(256, activation='relu', name=\"Layer-2\")(x)\n",
    "# [Final layers] dropout layer\n",
    "x = layers.Dropout(0.5, name=\"Layer-1\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "outputs = layers.Dense(1, activation='softmax', name=\"Output_Layer\")(x)\n",
    "res_net_model = keras.Model(inputs, outputs)\n",
    "\n",
    "res_net_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbai/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 43s 202ms/step - loss: 16840.2930 - acc: 0.1081 - val_loss: 430.5132 - val_acc: 0.2917\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 7787.7861 - acc: 0.0916 - val_loss: 2420.3792 - val_acc: 0.2917\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 4769.9448 - acc: 0.0797 - val_loss: 243.5034 - val_acc: 0.2917\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 38s 191ms/step - loss: 5470.1958 - acc: 0.0828 - val_loss: 66.2723 - val_acc: 0.2917\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 6677.0132 - acc: 0.0791 - val_loss: 20372.6543 - val_acc: 0.2917\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 2620.1509 - acc: 0.0744 - val_loss: 3864.3574 - val_acc: 0.2917\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 36760.1602 - acc: 0.0781 - val_loss: 10508.8252 - val_acc: 0.2917\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 11808.9072 - acc: 0.0916 - val_loss: 154.5165 - val_acc: 0.2917\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 7592.7876 - acc: 0.1063 - val_loss: 29.3578 - val_acc: 0.2917\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 39s 193ms/step - loss: 16433.7539 - acc: 0.1181 - val_loss: 12806.9844 - val_acc: 0.2917\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 29522.9707 - acc: 0.1241 - val_loss: 54.6965 - val_acc: 0.2917\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 4243.4497 - acc: 0.1322 - val_loss: 305.1541 - val_acc: 0.2917\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 942.1882 - acc: 0.1284 - val_loss: 104.3551 - val_acc: 0.2917\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 167.5469 - acc: 0.1359 - val_loss: 1369.2938 - val_acc: 0.2917\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 2826.5857 - acc: 0.1273 - val_loss: 76.0180 - val_acc: 0.2917\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 716.9807 - acc: 0.0969 - val_loss: 43.7505 - val_acc: 0.2917\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 88.7092 - acc: 0.0825 - val_loss: 102.3197 - val_acc: 0.2917\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 654.6701 - acc: 0.0825 - val_loss: 52.8290 - val_acc: 0.2917\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 167.5674 - acc: 0.0772 - val_loss: 47.2359 - val_acc: 0.2917\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 1177.6031 - acc: 0.0794 - val_loss: 31.4079 - val_acc: 0.2917\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 51.0661 - acc: 0.0759 - val_loss: 631.1693 - val_acc: 0.2917\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 528.6725 - acc: 0.0862 - val_loss: 384.0500 - val_acc: 0.2917\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 481.9288 - acc: 0.0984 - val_loss: 1850.9927 - val_acc: 0.2917\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 776.3979 - acc: 0.1141 - val_loss: 1234.9308 - val_acc: 0.2917\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 1007.2084 - acc: 0.1206 - val_loss: 183.2141 - val_acc: 0.2917\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 384.7350 - acc: 0.1297 - val_loss: 155.3326 - val_acc: 0.2917\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 249.7833 - acc: 0.1275 - val_loss: 13.2502 - val_acc: 0.2917\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 56.1117 - acc: 0.1359 - val_loss: 513.9587 - val_acc: 0.2917\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 131.5612 - acc: 0.1373 - val_loss: 143.3197 - val_acc: 0.2917\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 856.0751 - acc: 0.1034 - val_loss: 91.2964 - val_acc: 0.2917\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 38s 192ms/step - loss: 373.3969 - acc: 0.0909 - val_loss: 39.2031 - val_acc: 0.2917\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 46.8982 - acc: 0.0778 - val_loss: 51.6927 - val_acc: 0.2917\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 55.5524 - acc: 0.0838 - val_loss: 91.8646 - val_acc: 0.2917\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 196.7325 - acc: 0.0766 - val_loss: 6.7572 - val_acc: 0.2917\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 13.1055 - acc: 0.0750 - val_loss: 36.9805 - val_acc: 0.2917\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 15.2560 - acc: 0.0822 - val_loss: 98.7812 - val_acc: 0.2917\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 24.7922 - acc: 0.0934 - val_loss: 147.0737 - val_acc: 0.2917\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 38s 190ms/step - loss: 8.9709 - acc: 0.1081 - val_loss: 119.9090 - val_acc: 0.2917\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 1227.9861 - acc: 0.1187 - val_loss: 150.6153 - val_acc: 0.2917\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 2073.3267 - acc: 0.1256 - val_loss: 20.3209 - val_acc: 0.2917\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 652.2664 - acc: 0.1322 - val_loss: 11.1068 - val_acc: 0.2917\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 426.4782 - acc: 0.1281 - val_loss: 203.7590 - val_acc: 0.2917\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 39s 196ms/step - loss: 442.0989 - acc: 0.1428 - val_loss: 27.0749 - val_acc: 0.2917\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 75.8592 - acc: 0.1182 - val_loss: 8.1174 - val_acc: 0.2917\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 39s 197ms/step - loss: 152.7595 - acc: 0.0919 - val_loss: 11.3224 - val_acc: 0.2917\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 35.3067 - acc: 0.0816 - val_loss: 31.2438 - val_acc: 0.2917\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 39.2836 - acc: 0.0803 - val_loss: 25.7224 - val_acc: 0.2917\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 29.1679 - acc: 0.0800 - val_loss: 56.8663 - val_acc: 0.2917\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 301.2384 - acc: 0.0791 - val_loss: 3.7118 - val_acc: 0.2917\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 56.7201 - acc: 0.0750 - val_loss: 8.3484 - val_acc: 0.2917\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 130.8035 - acc: 0.0872 - val_loss: 7.3397 - val_acc: 0.2917\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 94.2973 - acc: 0.1034 - val_loss: 4.5051 - val_acc: 0.2917\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 45.9727 - acc: 0.1163 - val_loss: 50.7761 - val_acc: 0.2917\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 54.8673 - acc: 0.1225 - val_loss: 8.2406 - val_acc: 0.2917\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 9.7134 - acc: 0.1303 - val_loss: 11.4000 - val_acc: 0.2917\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 15.5850 - acc: 0.1287 - val_loss: 15.9665 - val_acc: 0.2917\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 83.4535 - acc: 0.1356 - val_loss: 21.5740 - val_acc: 0.2917\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 2.5034 - acc: 0.1316 - val_loss: 24.0572 - val_acc: 0.2917\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 3.2184 - acc: 0.1031 - val_loss: 25.4100 - val_acc: 0.2917\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 4.2917 - acc: 0.0838 - val_loss: 5.8973 - val_acc: 0.2917\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 6.5828 - acc: 0.0825 - val_loss: 3.1907 - val_acc: 0.2917\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 3.5580 - acc: 0.0809 - val_loss: 2.2991 - val_acc: 0.2917\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 1.8888 - acc: 0.0775 - val_loss: 6.0679 - val_acc: 0.2917\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 1.8361 - acc: 0.0725 - val_loss: 5.7317 - val_acc: 0.2917\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 10.5633 - acc: 0.0841 - val_loss: 3.7478 - val_acc: 0.2917\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 4.1105 - acc: 0.0956 - val_loss: 7.5927 - val_acc: 0.2917\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 1.3453 - acc: 0.1122 - val_loss: 6.6816 - val_acc: 0.2917\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 75.3502 - acc: 0.1184 - val_loss: 3.4698 - val_acc: 0.2917\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 119.9525 - acc: 0.1312 - val_loss: 35.3156 - val_acc: 0.2917\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 348.2077 - acc: 0.1275 - val_loss: 728.6948 - val_acc: 0.2917\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 23.6051 - acc: 0.1316 - val_loss: 178.3580 - val_acc: 0.2917\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 445.5974 - acc: 0.1412 - val_loss: 22.4632 - val_acc: 0.2917\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 176.8877 - acc: 0.1107 - val_loss: 8.8320 - val_acc: 0.2917\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 69.4058 - acc: 0.0928 - val_loss: 9.6934 - val_acc: 0.2917\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 82.7232 - acc: 0.0806 - val_loss: 2.4398 - val_acc: 0.2917\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 69.6342 - acc: 0.0812 - val_loss: 20.3077 - val_acc: 0.2917\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 127.3171 - acc: 0.0772 - val_loss: 11.4312 - val_acc: 0.2917\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 21.9912 - acc: 0.0763 - val_loss: 1.4399 - val_acc: 0.2917\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 120.2927 - acc: 0.0800 - val_loss: 41.7126 - val_acc: 0.2917\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 13.3861 - acc: 0.0897 - val_loss: 18.8593 - val_acc: 0.2917\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 620.4384 - acc: 0.1047 - val_loss: 1.4277 - val_acc: 0.2917\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 645.2242 - acc: 0.1178 - val_loss: 1.4502 - val_acc: 0.2917\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 561.4700 - acc: 0.1247 - val_loss: 1.0389 - val_acc: 0.2917\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 1306.5236 - acc: 0.1322 - val_loss: 1.0160 - val_acc: 0.2917\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 119.9525 - acc: 0.1284 - val_loss: 0.7363 - val_acc: 0.2917\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 31.9580 - acc: 0.1350 - val_loss: 0.7155 - val_acc: 0.2917\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 31.2407 - acc: 0.1279 - val_loss: 0.7159 - val_acc: 0.2917\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 10.9700 - acc: 0.0991 - val_loss: 0.7509 - val_acc: 0.2917\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 305.9218 - acc: 0.0816 - val_loss: 0.8410 - val_acc: 0.2917\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 36.6987 - acc: 0.0825 - val_loss: 0.9898 - val_acc: 0.2917\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 2.4570 - acc: 0.0781 - val_loss: 0.7674 - val_acc: 0.2917\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 23.5508 - acc: 0.0797 - val_loss: 0.7775 - val_acc: 0.2917\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 9.3040 - acc: 0.0756 - val_loss: 0.7720 - val_acc: 0.2917\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 51.1661 - acc: 0.0856 - val_loss: 0.7546 - val_acc: 0.2917\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 89.5691 - acc: 0.0981 - val_loss: 0.7677 - val_acc: 0.2917\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 84.4120 - acc: 0.1141 - val_loss: 0.7460 - val_acc: 0.2917\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 249.6596 - acc: 0.1175 - val_loss: 0.7472 - val_acc: 0.2917\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 65.1670 - acc: 0.1306 - val_loss: 0.7174 - val_acc: 0.2917\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 0.4917 - acc: 0.1284 - val_loss: 0.7114 - val_acc: 0.2917\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 0.4441 - acc: 0.1372 - val_loss: 0.6941 - val_acc: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f25707f5790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  keras.callbacks.TensorBoard(log_dir='./log/{}'.format(\n",
    "      dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S ResNet\")), write_images=True),\n",
    "  ]\n",
    "res_net_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "res_net_model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}