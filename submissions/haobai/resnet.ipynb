{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集概览\n",
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_all_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset()\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集\n",
    "### City A (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"source labeled (City A weak & failure):\", X_train.source.shape)\n",
    "# print(np.where(np.isnan(X_train.source[0])))\n",
    "print(\"|- labels:\", y_train.source.shape)\n",
    "print(\" |- weak=0:\", y_train.source[y_train.source==0].shape)\n",
    "print(\" |- failure=1:\", y_train.source[y_train.source==1].shape)\n",
    "print(\"source background (City A good):\", X_train.source_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_train.target.shape)\n",
    "print(\"|- labels:\", y_train.target.shape)\n",
    "print(\" |- weak=0:\", y_train.target[y_train.target==0].shape)\n",
    "print(\" |- failure=1:\", y_train.target[y_train.target==1].shape)\n",
    "print(\"target background (City B good):\", X_train.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集\n",
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_test.target.shape)\n",
    "print(\"|- labels:\", y_test.target.shape)\n",
    "print(\" |- weak=0:\", y_test.target[y_test.target==0].shape)\n",
    "print(\" |- failure=1:\", y_test.target[y_test.target==1].shape)\n",
    "print(\"target background (City B good):\", X_test.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network\n",
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除NaN\n",
    "from numpy import newaxis\n",
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Replace NaN by 0 and flatten the matrix to size (sample, 6720).\n",
    "        Executed on every input data (i.e., source, bkg, target) and passed\n",
    "        the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        return X[:,:,:,newaxis]  \n",
    "\n",
    "fe = FeatureExtractor()\n",
    "X_train.source = fe.transform(X_train.source)\n",
    "X_train.target = fe.transform(X_train.target)\n",
    "X_test.target = fe.transform(X_test.target)\n",
    "X_train.source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集转换为TensorFlow格式\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.source, y_train.source)).batch(16)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_train.target, y_train.target)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target))\n",
    "\n",
    "# 额外操作\n",
    "train_dataset = train_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) )\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络模型\n",
    "参考资料\n",
    "\n",
    "1. [Introduction to ResNet in TensorFlow 2](https://adventuresinmachinelearning.com/introduction-resnet-tensorflow-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(filters, conv_size, input_data):\n",
    "    ''' A residual block of 3 layers\n",
    "    '''\n",
    "    # 1st layer with batch normalization\n",
    "    x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 2nd layer with batch normalization, but no activation function\n",
    "    x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 3rd layer is residual addition with an activation function\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to be used as an entry point into a Network (a graph of layers).\n",
    "# https://keras.io/api/layers/core_layers/input/\n",
    "inputs = keras.Input(shape=(672, 10, 1), name=\"Input_Layer\")\n",
    "# inputs = layers.InputLayer(input_shape=(672, 10), name=\"Input_Layer\")\n",
    "\n",
    "# 2D convolution layer (e.g. spatial convolution over images).\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "x = layers.Conv2D(32, 2, activation='relu', name=\"Layer01\")(inputs)\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer02\")(x)\n",
    "\n",
    "# Max pooling operation for 2D spatial data.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "# pool_size: integer or tuple of 2 integers, window size over which to take the maximum.\n",
    "x = layers.MaxPooling2D(2, name=\"Layer03\")(x)\n",
    "\n",
    "num_res_net_blocks = 10 # 10个ResNet blocks\n",
    "for i in range(num_res_net_blocks):\n",
    "    x = res_net_block(64, 2, x)\n",
    "\n",
    "# [Final layers] a standard CNN layer\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer-4\")(x)\n",
    "# [Final layers] GAP layer\n",
    "x = layers.GlobalAveragePooling2D(name=\"Layer-3\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "# Just your regular densely-connected NN layer.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "# units: Positive integer, dimensionality of the output space.\n",
    "x = layers.Dense(256, activation='relu', name=\"Layer-2\")(x)\n",
    "# [Final layers] dropout layer\n",
    "x = layers.Dropout(0.5, name=\"Layer-1\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "outputs = layers.Dense(1, activation='softmax', name=\"Output_Layer\")(x)\n",
    "res_net_model = keras.Model(inputs, outputs)\n",
    "\n",
    "res_net_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "'binary_crossentropy' 效果比 'categorical_crossentropy' 稍好。后者计算的loss直接=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 44s 206ms/step - loss: 28853349122048.0000 - acc: 0.1081 - val_loss: 14470327828480.0000 - val_acc: 0.2917\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 39866876297216.0000 - acc: 0.0916 - val_loss: 12976597762048.0000 - val_acc: 0.2917\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 19106625486848.0000 - acc: 0.0797 - val_loss: 11967801589760.0000 - val_acc: 0.2917\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 35934275895296.0000 - acc: 0.0828 - val_loss: 10949795774464.0000 - val_acc: 0.2917\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 16959125061632.0000 - acc: 0.0791 - val_loss: 9528608292864.0000 - val_acc: 0.2917\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 14154781949952.0000 - acc: 0.0744 - val_loss: 9088081592320.0000 - val_acc: 0.2917\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 41s 202ms/step - loss: 23153638113280.0000 - acc: 0.0781 - val_loss: 8168037941248.0000 - val_acc: 0.2917\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 11540884357120.0000 - acc: 0.0916 - val_loss: 7593860792320.0000 - val_acc: 0.2917\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 27810989080576.0000 - acc: 0.1063 - val_loss: 6580801634304.0000 - val_acc: 0.2917\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 14934023864320.0000 - acc: 0.1181 - val_loss: 6134157541376.0000 - val_acc: 0.2917\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 19599076622336.0000 - acc: 0.1241 - val_loss: 5504258015232.0000 - val_acc: 0.2917\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 10449552670720.0000 - acc: 0.1322 - val_loss: 5197496057856.0000 - val_acc: 0.2917\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 14890571923456.0000 - acc: 0.1284 - val_loss: 4492019367936.0000 - val_acc: 0.2917\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 6767249981440.0000 - acc: 0.1359 - val_loss: 4300539428864.0000 - val_acc: 0.2917\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 13194601955328.0000 - acc: 0.1273 - val_loss: 3811134668800.0000 - val_acc: 0.2917\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 5956285497344.0000 - acc: 0.0969 - val_loss: 3517294313472.0000 - val_acc: 0.2917\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 10459798306816.0000 - acc: 0.0825 - val_loss: 3198670602240.0000 - val_acc: 0.2917\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 5374463705088.0000 - acc: 0.0825 - val_loss: 2982493814784.0000 - val_acc: 0.2917\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 3401185492992.0000 - acc: 0.0772 - val_loss: 2814133665792.0000 - val_acc: 0.2917\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 8422646349824.0000 - acc: 0.0794 - val_loss: 2484216528896.0000 - val_acc: 0.2917\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 3810596225024.0000 - acc: 0.0759 - val_loss: 2297042305024.0000 - val_acc: 0.2917\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 6801822056448.0000 - acc: 0.0862 - val_loss: 2000068411392.0000 - val_acc: 0.2917\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 4444714434560.0000 - acc: 0.0984 - val_loss: 1844612300800.0000 - val_acc: 0.2917\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 7025713479680.0000 - acc: 0.1141 - val_loss: 1589695741952.0000 - val_acc: 0.2917\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 3722892541952.0000 - acc: 0.1206 - val_loss: 1424394420224.0000 - val_acc: 0.2917\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 4277082521600.0000 - acc: 0.1297 - val_loss: 1221575704576.0000 - val_acc: 0.2917\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 2240227049472.0000 - acc: 0.1275 - val_loss: 1123510910976.0000 - val_acc: 0.2917\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 3771788951552.0000 - acc: 0.1359 - val_loss: 1018500218880.0000 - val_acc: 0.2917\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 1744928899072.0000 - acc: 0.1373 - val_loss: 933101764608.0000 - val_acc: 0.2917\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 3213399687168.0000 - acc: 0.1034 - val_loss: 844332728320.0000 - val_acc: 0.2917\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 1196791693312.0000 - acc: 0.0909 - val_loss: 748427673600.0000 - val_acc: 0.2917\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 1496903319552.0000 - acc: 0.0778 - val_loss: 701844881408.0000 - val_acc: 0.2917\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 2314068557824.0000 - acc: 0.0838 - val_loss: 630271901696.0000 - val_acc: 0.2917\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 1155149463552.0000 - acc: 0.0766 - val_loss: 564793573376.0000 - val_acc: 0.2917\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 1751145906176.0000 - acc: 0.0750 - val_loss: 493931593728.0000 - val_acc: 0.2917\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 660470824960.0000 - acc: 0.0822 - val_loss: 445545349120.0000 - val_acc: 0.2917\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 1534665162752.0000 - acc: 0.0934 - val_loss: 390431604736.0000 - val_acc: 0.2917\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 975903129600.0000 - acc: 0.1081 - val_loss: 344831000576.0000 - val_acc: 0.2917\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 1125254823936.0000 - acc: 0.1187 - val_loss: 276920729600.0000 - val_acc: 0.2917\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 659239993344.0000 - acc: 0.1256 - val_loss: 247370088448.0000 - val_acc: 0.2917\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 881726652416.0000 - acc: 0.1322 - val_loss: 193971208192.0000 - val_acc: 0.2917\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 317503897600.0000 - acc: 0.1281 - val_loss: 169858236416.0000 - val_acc: 0.2917\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 748127846400.0000 - acc: 0.1428 - val_loss: 150073196544.0000 - val_acc: 0.2917\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 243885867008.0000 - acc: 0.1182 - val_loss: 124109168640.0000 - val_acc: 0.2917\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 255074697216.0000 - acc: 0.0919 - val_loss: 112758317056.0000 - val_acc: 0.2917\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 321693286400.0000 - acc: 0.0816 - val_loss: 90473816064.0000 - val_acc: 0.2917\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 279387930624.0000 - acc: 0.0803 - val_loss: 79097692160.0000 - val_acc: 0.2917\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 322231304192.0000 - acc: 0.0800 - val_loss: 55871057920.0000 - val_acc: 0.2917\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 64954212352.0000 - acc: 0.0791 - val_loss: 43425001472.0000 - val_acc: 0.2917\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 173053935616.0000 - acc: 0.0750 - val_loss: 36264587264.0000 - val_acc: 0.2917\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 246870900736.0000 - acc: 0.0872 - val_loss: 24996970496.0000 - val_acc: 0.2917\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 105844015104.0000 - acc: 0.1034 - val_loss: 15695432704.0000 - val_acc: 0.2917\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 53262012416.0000 - acc: 0.1163 - val_loss: 8191488512.0000 - val_acc: 0.2917\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 49716363264.0000 - acc: 0.1225 - val_loss: 3948948224.0000 - val_acc: 0.2917\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 10794995712.0000 - acc: 0.1303 - val_loss: 1819913344.0000 - val_acc: 0.2917\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 63188910080.0000 - acc: 0.1287 - val_loss: 2746411264.0000 - val_acc: 0.2917\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 27479320576.0000 - acc: 0.1356 - val_loss: 52074228.0000 - val_acc: 0.2917\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 29475973120.0000 - acc: 0.1316 - val_loss: 2783228160.0000 - val_acc: 0.2917\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 25533313024.0000 - acc: 0.1031 - val_loss: 1144427392.0000 - val_acc: 0.2917\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 10086451200.0000 - acc: 0.0838 - val_loss: 935444288.0000 - val_acc: 0.2917\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 5612843008.0000 - acc: 0.0825 - val_loss: 2277406720.0000 - val_acc: 0.2917\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 12008509440.0000 - acc: 0.0809 - val_loss: 1650982912.0000 - val_acc: 0.2917\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 29502959616.0000 - acc: 0.0775 - val_loss: 1988842112.0000 - val_acc: 0.2917\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 11792299008.0000 - acc: 0.0725 - val_loss: 1187627648.0000 - val_acc: 0.2917\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 5931117568.0000 - acc: 0.0841 - val_loss: 516942720.0000 - val_acc: 0.2917\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 24833900544.0000 - acc: 0.0956 - val_loss: 667648704.0000 - val_acc: 0.2917\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 26201462784.0000 - acc: 0.1122 - val_loss: 4285714432.0000 - val_acc: 0.2917\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 41300692992.0000 - acc: 0.1184 - val_loss: 7413164032.0000 - val_acc: 0.2917\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 31103219712.0000 - acc: 0.1312 - val_loss: 13676286976.0000 - val_acc: 0.2917\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 33238695936.0000 - acc: 0.1275 - val_loss: 16021292032.0000 - val_acc: 0.2917\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 29659938816.0000 - acc: 0.1316 - val_loss: 8714105856.0000 - val_acc: 0.2917\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 29407332352.0000 - acc: 0.1412 - val_loss: 8027006464.0000 - val_acc: 0.2917\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 42s 209ms/step - loss: 5483231232.0000 - acc: 0.1107 - val_loss: 5040472064.0000 - val_acc: 0.2917\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 17986457600.0000 - acc: 0.0928 - val_loss: 189736496.0000 - val_acc: 0.2917\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 9824481280.0000 - acc: 0.0806 - val_loss: 571964480.0000 - val_acc: 0.2917\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 6670617600.0000 - acc: 0.0812 - val_loss: 457059424.0000 - val_acc: 0.2917\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 15922002944.0000 - acc: 0.0772 - val_loss: 1093975936.0000 - val_acc: 0.2917\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 7380440064.0000 - acc: 0.0763 - val_loss: 781878848.0000 - val_acc: 0.2917\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 13710217216.0000 - acc: 0.0800 - val_loss: 1036405248.0000 - val_acc: 0.2917\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 6720781824.0000 - acc: 0.0897 - val_loss: 836766656.0000 - val_acc: 0.2917\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 10929592320.0000 - acc: 0.1047 - val_loss: 368963488.0000 - val_acc: 0.2917\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 10883526656.0000 - acc: 0.1178 - val_loss: 50453776.0000 - val_acc: 0.2917\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 8403726336.0000 - acc: 0.1247 - val_loss: 179184208.0000 - val_acc: 0.2917\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 10827833344.0000 - acc: 0.1322 - val_loss: 34379796.0000 - val_acc: 0.2917\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 6515325440.0000 - acc: 0.1284 - val_loss: 232608944.0000 - val_acc: 0.2917\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 2465588992.0000 - acc: 0.1350 - val_loss: 129261696.0000 - val_acc: 0.2917\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 40s 202ms/step - loss: 8977525760.0000 - acc: 0.1279 - val_loss: 61727772.0000 - val_acc: 0.2917\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 4087734528.0000 - acc: 0.0991 - val_loss: 111537152.0000 - val_acc: 0.2917\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 42s 210ms/step - loss: 671773248.0000 - acc: 0.0816 - val_loss: 60836384.0000 - val_acc: 0.2917\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 42s 208ms/step - loss: 2037111680.0000 - acc: 0.0825 - val_loss: 194087504.0000 - val_acc: 0.2917\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 375659648.0000 - acc: 0.0781 - val_loss: 185356608.0000 - val_acc: 0.2917\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 1318646144.0000 - acc: 0.0797 - val_loss: 208304912.0000 - val_acc: 0.2917\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 952769984.0000 - acc: 0.0756 - val_loss: 176016656.0000 - val_acc: 0.2917\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 2783510528.0000 - acc: 0.0856 - val_loss: 202645968.0000 - val_acc: 0.2917\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 2559913472.0000 - acc: 0.0981 - val_loss: 147268544.0000 - val_acc: 0.2917\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 3353632000.0000 - acc: 0.1141 - val_loss: 53888508.0000 - val_acc: 0.2917\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 1944627584.0000 - acc: 0.1175 - val_loss: 8127061.5000 - val_acc: 0.2917\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 41s 205ms/step - loss: 2137104896.0000 - acc: 0.1306 - val_loss: 59820340.0000 - val_acc: 0.2917\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 1341730176.0000 - acc: 0.1284 - val_loss: 110010240.0000 - val_acc: 0.2917\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 2108838912.0000 - acc: 0.1372 - val_loss: 32593392.0000 - val_acc: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b3012e370>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./log/{}'.format(\n",
    "      dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S ResNet\")), write_images=True),\n",
    "  ]\n",
    "res_net_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "res_net_model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000],\n",
       "       [1.000],\n",
       "       [1.000],\n",
       "       ...,\n",
       "       [1.000],\n",
       "       [1.000],\n",
       "       [1.000]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_net_model.predict(X_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}