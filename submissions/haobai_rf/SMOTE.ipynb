{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Minority Oversampling Technique (SMOTE) for Imbalanced classification\n",
    "1. [Blog: SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "\n",
    "## 预处理\n",
    "### 装载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除NaN\n",
    "from numpy import newaxis\n",
    "class FeatureExtractor:\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        return X\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理数据（Normalization, Oversampling, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as il\n",
    "from collections import Counter\n",
    "over = il.over_sampling.RandomOverSampler(sampling_strategy=self.sampling_strategy,) # 0.5 is the best for random state 44, 0.3 is generally the best for all\n",
    "# over = il.over_sampling.ADASYN(sampling_strategy=self.sampling_strategy) # 0.2 is the best\n",
    "# over = il.over_sampling.BorderlineSMOTE(sampling_strategy=self.sampling_strategy) # 0.2 is the best\n",
    "# over = il.over_sampling.KMeansSMOTE(sampling_strategy=self.sampling_strategy) # error\n",
    "# over = il.over_sampling.SMOTE(sampling_strategy=self.sampling_strategy) # 0.2\n",
    "# over = il.over_sampling.SMOTENC((0,0,0,0,0,0,0,0,0,0), sampling_strategy=self.sampling_strategy) #\n",
    "# over = il.over_sampling.SVMSMOTE() # abandon, long training time\n",
    "X_source, y_source = over.fit_resample(X_source, y_source)\n",
    "\n",
    "# under = il.under_sampling.RandomUnderSampler(sampling_strategy=1.0)\n",
    "# X_source, y_source = under.fit_resample(X_source, y_source)\n",
    "\n",
    "# over = il.over_sampling.SVMSMOTE() # abandon, long training time\n",
    "over = il.over_sampling.BorderlineSMOTE()\n",
    "X_source, y_source = over.fit_resample(X_source, y_source)\n",
    "\n",
    "print(X_source.shape, y_source.shape)\n",
    "print(Counter(y_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_DT = DecisionTreeClassifier(max_depth=2, random_state=44,)\n",
    "model_RF = RandomForestClassifier(\n",
    "    n_estimators=2, max_depth=2, random_state=44, n_jobs=-1)\n",
    "\n",
    "# define pipeline\n",
    "# over = il.over_sampling.SMOTE(sampling_strategy=0.1)\n",
    "# under = il.under_sampling.RandomUnderSampler(sampling_strategy=0.5)\n",
    "# steps = [('over', over), ('under', under), ('model', model_DT)]\n",
    "# pipeline_DT = il.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "# steps = [('over', over), ('under', under), ('model', model_RF)]\n",
    "# pipeline_RF = il.pipeline.Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT.fit(X_source, y_source)\n",
    "model_RF.fit(X_source, y_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "y_pred = model_DT.predict(X_test.target)\n",
    "print(\"[Target] Decision Tree:\", model_DT.score(X_target, y_target))\n",
    "print(\"[Target] Decision Tree:\", model_DT.score(X_test.target, y_test.target))\n",
    "print(\"Predicted:\", Counter(y_pred), y_pred.shape)\n",
    "print(\"True:      \", Counter(y_test.target), y_test.target.shape)\n",
    "\n",
    "print(\"Random Forest:\", model_RF.score(X_target, y_target))\n",
    "print(\"Random Forest:\", model_RF.score(X_test.target, y_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试超参\n",
    "直接使用华为提供的比赛工具包来评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.2: Bagged ap score = 0.18577221956242598\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.3: Bagged ap score = 0.18272019978403817\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.4: Bagged ap score = 0.18195265719671538\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.5: Bagged ap score = 0.17747303981613757\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.6: Bagged ap score = 0.17256657035777528\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.7: Bagged ap score = 0.17258657703842828\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.8: Bagged ap score = 0.17475288065806577\n",
      "Train data\n",
      "Test data\n",
      "over 0.2, under 0.9: Bagged ap score = 0.16778444152133087\n",
      "Train data\n",
      "Test data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-149d7f29d32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_is\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_is\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             trained_workflow = problem.workflow.train_submission(\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_is\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0msampling_strategy_over\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mss1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             fe, X.target_bkg)\n\u001b[1;32m     77\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpticalLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             sanitize=False)\n\u001b[1;32m     35\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         clf.fit(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_bkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             X.target_bkg, y.source, y.target)\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/submissions/haobai_rf/./classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg, y_source, y_target)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# under = il.under_sampling.AllKNN(sampling_strategy=self.sampling_strategy) # ValueError: 'clean-sampling' methods do let the user specify the sampling ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# under = il.under_sampling.TomekLinks(sampling_strategy=self.sampling_strategy) # long time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mX_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         return OrderedDict(\n\u001b[1;32m    534\u001b[0m             sorted(\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0m_sampling_strategy_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m             )\n\u001b[1;32m    537\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_float\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    391\u001b[0m             ]\n\u001b[1;32m    392\u001b[0m         ):\n\u001b[0;32m--> 393\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    394\u001b[0m                 \u001b[0;34m\"The specified ratio required to generate new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;34m\"sample in the majority class while trying to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio."
     ]
    }
   ],
   "source": [
    "import rampwf as rw\n",
    "import numpy as np\n",
    "from others import cd\n",
    "\n",
    "hp_range = [i/10 for i in range(2, 10, 1)]\n",
    "# hp_range = ['not minority', 'not majority', 'all', 'majority',]\n",
    "ap_bagged_test = []\n",
    "for ss1 in hp_range:\n",
    "    for ss2 in hp_range:\n",
    "        # 载入数据\n",
    "        with cd(\"~/Codes/HuaweiRAMP\"):\n",
    "            problem = rw.utils.assert_read_problem()\n",
    "            X_train, y_train = problem.get_train_data(show=False)\n",
    "            X_test, y_test = problem.get_test_data(show=False)\n",
    "        # 导入评价函数\n",
    "        ap    = problem.score_types[0]\n",
    "        # 设置crossvalidation\n",
    "        splits = problem.get_cv(X_train, y_train, n_splits=10) # 默认10\n",
    "        # 开始实验\n",
    "        ap_train, ap_valid, ap_test, = [], [], []\n",
    "        y_test_preds = []\n",
    "        for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "            with cd(\"~/Codes/HuaweiRAMP\"):\n",
    "                X_test, y_test = problem.get_test_data(show=False)\n",
    "            trained_workflow = problem.workflow.train_submission(\n",
    "                '.', X_train, y_train, train_is,\n",
    "                sampling_strategy_over=ss1,\n",
    "                sampling_strategy_under=ss2,\n",
    "                )\n",
    "            X_fold_train = X_train.slice(train_is)\n",
    "            X_fold_valid = X_train.slice(valid_is)\n",
    "            \n",
    "            y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "            y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "            y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "            ap_train.append( ap(y_train.slice(train_is).target, y_train_pred[:,1]) )\n",
    "            ap_valid.append( ap(y_train.slice(valid_is).target, y_valid_pred[:,1]) )\n",
    "            ap_test.append( ap(y_test.target, y_test_pred[:,1]) )\n",
    "            # print('-------------------------------------')\n",
    "            # print('training ap on fold {} = {:.3f}'.format(fold_i, ap_train[-1]))\n",
    "            # print('validation ap on fold {} = {:.3f}'.format(fold_i, ap_valid[-1]))\n",
    "            # print('test ap on fold {} = {:.3f}'.format(fold_i, ap_test[-1]))\n",
    "            \n",
    "            y_test_preds.append(y_test_pred)\n",
    "\n",
    "        # 计算排名指标: bagged average precision on test dataset\n",
    "        score = ap(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1])\n",
    "        ap_bagged_test.append({\"over\":ss1, \"under\":ss2, \"score\":score,\n",
    "            \"ap_train\":ap_train, \"ap_valid\":ap_valid, \"ap_test\":ap_test})\n",
    "        print('over {}, under {}: Bagged ap score = {}'.format(ss1, ss2, score))\n",
    "        del problem, X_train, y_train, X_test, y_test, ap, splits, y_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"RandomOverUnder.json\", \"w\") as f:\n",
    "    json.dump(ap_bagged_test, f)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots()\n",
    "fig.add_trace(go.Scatter(x=hp_range, y=ap_bagged_test, mode=\"lines\",\n",
    "                         name=\"sampling_strategy\", showlegend=True))\n",
    "fig.update_layout(\n",
    "    title=\"RandomOverSampler+RandomUnderSampler: s\",\n",
    "    xaxis_title=\"Hyperparameter\",\n",
    "    yaxis_title=\"Bagged ap\",\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}