{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理探究\n",
    "## 装载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0(0.00%) samples have all NaN, 341(77.85%) samples have partial NaN.\n",
      "All NaN columns: set() Partial NaN columns: {0, 1} Dropped samples: 97(22.15%)\n",
      "==== TRAIN SET ====\n",
      "\t0(0.00%) samples have all NaN, 18200(39.47%) samples have partial NaN.\n",
      "All NaN columns: set() Partial NaN columns: {0} Dropped samples: 7783(16.88%)\n",
      "  | X_source: (38327, 672, 9) ; y_source: (46110,)\n",
      "\t79(0.16%) samples have all NaN, 17969(35.33%) samples have partial NaN.\n",
      "All NaN columns: {0} Partial NaN columns: {0} Dropped samples: 14660(28.82%)\n",
      "A | X_source_bkg: (36202, 672, 9)\n",
      "\t0(0.00%) samples have all NaN, 341(77.85%) samples have partial NaN.\n",
      "All NaN columns: set() Partial NaN columns: {0, 1} Dropped samples: 97(22.15%)\n",
      "----\n",
      "  | X_target: (341, 672, 9) ; y_target: (438,)\n",
      "\t1094(3.70%) samples have all NaN, 21663(73.21%) samples have partial NaN.\n",
      "All NaN columns: {0, 1} Partial NaN columns: {0, 1} Dropped samples: 7897(26.69%)\n",
      "B | X_target_bkg: (21695, 672, 9)\n",
      "\t11(0.13%) samples have all NaN, 6220(75.84%) samples have partial NaN.\n",
      "All NaN columns: {1} Partial NaN columns: {0, 1} Dropped samples: 1976(24.09%)\n",
      "  | X_target_unlabeled: (6226, 672, 9)\n",
      "==== TEST SET ====\n",
      "\t1(0.01%) samples have all NaN, 15702(88.42%) samples have partial NaN.\n",
      "All NaN columns: {1} Partial NaN columns: {0, 1} Dropped samples: 2056(11.58%)\n",
      "  | X_test.target: (15702, 672, 9) ; y_test.target: (17758,)\n",
      "\t749(1.58%) samples have all NaN, 39125(82.76%) samples have partial NaN.\n",
      "All NaN columns: {0, 1} Partial NaN columns: {0, 1} Dropped samples: 8144(17.23%)\n",
      "B | X_test.target_bkg: (39131, 672, 9)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "class FeatureExtractor:\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        #! ATTENTION\n",
    "        # The idea is supposed to eliminate the common columns filled entirely \n",
    "        # by NaN. But in this competition, since we don't have access to\n",
    "        # `OpticalDataset` object, it's impossible to communicate informations\n",
    "        # between datasets. So, here it deletes columns that are found on public\n",
    "        # dataset.\n",
    "        X = np.delete(X, [3,], axis=2)\n",
    "\n",
    "        new_X = []\n",
    "        # Deal with NaN\n",
    "        all_nan_col, part_nan = {}, {}\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            \n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(~np.isfinite(x), axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                col = np.where(col_is_nan == True)[0]\n",
    "                all_nan_col[i] = col[0]\n",
    "            \n",
    "            # Rows with full Nan\n",
    "            row_is_nan = np.all(~np.isfinite(x), axis=1)\n",
    "            if (row_is_nan == True).any():\n",
    "                row = np.where(row_is_nan == True)[0]\n",
    "                # x = np.delete(x, row, axis=0) # leads to diff dim\n",
    "                if len(row) >= x.shape[0]/4: # drop sample\n",
    "                    continue\n",
    "            \n",
    "            # TODO move to 2nd loop\n",
    "            # Columns with partial NaN\n",
    "            part_is_nan = np.any(~np.isfinite(x), axis=0)\n",
    "            if (part_is_nan == True).any():\n",
    "                col = np.where(part_is_nan == True)[0]\n",
    "                part_nan[i] = col[0]\n",
    "\n",
    "            \n",
    "            # Construct new array\n",
    "            new_X.append(x)\n",
    "        \n",
    "        print(\"\\t{}({:.2f}%) samples have all NaN, \"\n",
    "              \"{}({:.2f}%) samples have partial NaN.\".format(\n",
    "              len(all_nan_col), len(all_nan_col)/X.shape[0]*100,\n",
    "              len(part_nan), len(part_nan)/X.shape[0]*100))\n",
    "        diff = X.shape[0]-len(new_X)\n",
    "        print(\"All NaN columns:\", set(all_nan_col.values()),\n",
    "              \"Partial NaN columns:\", set(part_nan.values()),\n",
    "              \"Dropped samples: {}({:.2f}%)\".format(diff, diff/X.shape[0]*100,)\n",
    "             )\n",
    "        \n",
    "        XX = np.array(new_X)\n",
    "        return XX\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "X_target = fe.transform(X_train.target)\n",
    "np.all(np.isfinite(X_target)) # expected True\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, 672, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "\n",
    "Requirement:\n",
    "- input array must be 2D, i.e. (n_samples, n_features)\n",
    "- input array must not contain NaN, Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LocalOutlierFactor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-247be0d8f3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LocalOutlierFactor: 204s X_source_bkg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalOutlierFactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnovelty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LocalOutlierFactor' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "X = deepcopy(X_source_bkg)\n",
    "np.nan_to_num(X, copy=False)\n",
    "# LocalOutlierFactor: 204s X_source_bkg\n",
    "lof = LocalOutlierFactor(contamination=0.01, novelty=False)\n",
    "flag = lof.fit_predict(X)\n",
    "print(flag.shape)\n",
    "X[flag==True].shape\n",
    "# IsolationForest: 296s\n",
    "from sklearn.ensemble import IsolationForest\n",
    "IF = IsolationForest(contamination=0.01)\n",
    "flag = IF.fit_predict(X)\n",
    "print(flag.shape)\n",
    "X[flag==True].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理数据（Normalization, Oversampling, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop\n",
    "X_source = X_source.reshape(X_source.shape[0], -1)\n",
    "X_source_bkg = X_source_bkg.reshape(X_source_bkg.shape[0], -1)\n",
    "X_target = X_target.reshape(X_target.shape[0], -1)\n",
    "X_target_unlabeled = X_target_unlabeled.reshape(X_target_unlabeled.shape[0], -1)\n",
    "X_target_bkg = X_target_bkg.reshape(X_target_bkg.shape[0], -1)\n",
    "X_test.target = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "np.all(np.isfinite(X_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_DT = DecisionTreeClassifier(max_depth=2, random_state=44,)\n",
    "\n",
    "model_RF = RandomForestClassifier(\n",
    "    n_estimators=2, max_depth=2, random_state=44, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_DT.fit(X_source, y_source)\n",
    "model_RF.fit(X_source, y_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "\n",
    "print(\"[train: B] Random Forest:\", model_RF.score(X_target, y_target))\n",
    "print(\"[test:  B] Random Forest:\", model_RF.score(X_test.target, y_test.target))\n",
    "y_pred = model_RF.predict(X_test.target)\n",
    "print(\"Predicted:\", Counter(y_pred), y_pred.shape)\n",
    "print(\"True:      \", Counter(y_test.target), y_test.target.shape)\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调试超参\n",
    "直接使用华为提供的比赛工具包来评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rampwf as rw\n",
    "import numpy as np\n",
    "from others import cd\n",
    "\n",
    "hp_range = np.arange(0.2, 1.1, 0.1)\n",
    "ap_bagged_test = []\n",
    "\n",
    "for ss in hp_range:\n",
    "    # 载入数据\n",
    "    with cd(\"~/Codes/HuaweiRAMP\"):\n",
    "        problem = rw.utils.assert_read_problem()\n",
    "        X_train, y_train = problem.get_train_data(show=False)\n",
    "        X_test, y_test = problem.get_test_data(show=False)\n",
    "    # 导入评价函数\n",
    "    ap    = problem.score_types[0]\n",
    "    # 设置crossvalidation\n",
    "    splits = problem.get_cv(X_train, y_train, n_splits=10) # 默认10\n",
    "    # 开始实验\n",
    "    ap_train, ap_valid, ap_test, = [], [], []\n",
    "    y_test_preds = []\n",
    "    for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "        trained_workflow = problem.workflow.train_submission(\n",
    "            '.', X_train, y_train, train_is, sampling_strategy=ss)\n",
    "        X_fold_train = X_train.slice(train_is)\n",
    "        X_fold_valid = X_train.slice(valid_is)\n",
    "        \n",
    "        y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "        y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "        y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "        ap_train.append( ap(y_train.slice(train_is).target, y_train_pred[:,1]) )\n",
    "        ap_valid.append( ap(y_train.slice(valid_is).target, y_valid_pred[:,1]) )\n",
    "        ap_test.append( ap(y_test.target, y_test_pred[:,1]) )\n",
    "        # print('-------------------------------------')\n",
    "        # print('training ap on fold {} = {:.3f}'.format(fold_i, ap_train[-1]))\n",
    "        # print('validation ap on fold {} = {:.3f}'.format(fold_i, ap_valid[-1]))\n",
    "        # print('test ap on fold {} = {:.3f}'.format(fold_i, ap_test[-1]))\n",
    "        \n",
    "        y_test_preds.append(y_test_pred)\n",
    "\n",
    "    # 计算排名指标: bagged average precision on test dataset\n",
    "    ap_bagged_test.append(\n",
    "        ap(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1]))\n",
    "    print('{}: Bagged ap score = {}'.format(ss, ap_bagged_test[-1]))\n",
    "    del problem, X_train, y_train, X_test, y_test, ap, splits, y_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots()\n",
    "fig.add_trace(go.Scatter(x=hp_range, y=ap_bagged_test, mode=\"lines\",\n",
    "                         name=\"sampling_strategy\"))\n",
    "fig.update_layout(xaxis_title=\"Hyperparameter\", yaxis_title=\"Bagged ap\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}