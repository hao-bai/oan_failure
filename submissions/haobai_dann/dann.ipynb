{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Domain Adversarial Neural Network in Tensorflow](https://github.com/sghoshjr/Domain-Adversarial-Neural-Network/blob/master/DANN.py)\n",
    "\n",
    "# 预处理\n",
    "## 装载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TRAIN SET ====\n",
      "  | X_source: (322770, 96, 9, 1) ; y_source: (46110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-86c6dff1e429>:37: RuntimeWarning: Mean of empty slice\n",
      "  nanmean.append(np.nanmean(x, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A | X_source_bkg: (356034, 96, 9, 1)\n",
      "----\n",
      "  | X_target: (3066, 96, 9, 1) ; y_target: (438,)\n",
      "B | X_target_bkg: (207144, 96, 9, 1)\n",
      "  | X_target_unlabeled: (57414, 96, 9, 1)\n",
      "==== TEST SET ====\n",
      "  | X_test.target: (124306, 96, 9, 1) ; y_test.target: (17758,)\n",
      "B | X_test.target_bkg: (330925, 96, 9, 1)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "# from feature_extractor import FeatureExtractor\n",
    "from numpy import newaxis\n",
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Deal with NaN and flatten the matrix to size (sample, 6720).\n",
    "        Executed on every input data (i.e., source, bkg, target) and passed\n",
    "        the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        #! ATTENTION\n",
    "        # The idea is supposed to eliminate the common columns filled entirely \n",
    "        # by NaN. But in this competition, since we don't have access to\n",
    "        # `OpticalDataset` object, it's impossible to communicate informations\n",
    "        # between datasets. So, here it deletes columns that are found on public\n",
    "        # dataset.\n",
    "        X = np.delete(X, [3,], axis=2)\n",
    "        X = X.astype(np.float64)\n",
    "        \n",
    "        ## 1st round\n",
    "        X1, nanmean = [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            nanmean.append(np.nanmean(x, axis=0))\n",
    "\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                X1.append(x) # deal later\n",
    "                continue\n",
    "            \n",
    "            # Rows with full Nan\n",
    "            # Unachievable. Cause we don't have access to manipulate on labels\n",
    "            # row_is_nan = np.all(indice, axis=1)\n",
    "            # if (row_is_nan == True).any():\n",
    "            #     row = np.where(row_is_nan == True)[0]\n",
    "            #     if len(row) >= x.shape[0]/4: # drop sample, /2=85%+, /4=75%+\n",
    "            #         continue\n",
    "            \n",
    "            # Columns with partial NaN\n",
    "            part_is_nan = np.any(indice, axis=0)\n",
    "            if (part_is_nan == True).any():\n",
    "                col = np.where(part_is_nan == True)[0]\n",
    "                # part_nan[i] = col[0]\n",
    "                for c in col:\n",
    "                    this = x[:,c]\n",
    "                    finite = this[np.isfinite(this)]\n",
    "                    fill = np.repeat(finite, np.ceil(len(this)/len(finite)))[:len(this)]\n",
    "                    x[:,c] = np.where(np.isfinite(this), this, fill)\n",
    "            \n",
    "            # Construct new array\n",
    "            X1.append(x)\n",
    "        X1, nanmean = np.array(X1), np.array(nanmean)\n",
    "\n",
    "        ## 2nd round\n",
    "        candidate_mean = []\n",
    "        for i in range(nanmean.shape[1]):\n",
    "            col = nanmean[i]\n",
    "            finite = col[np.isfinite(col)]\n",
    "            candidate_mean.append(finite)\n",
    "\n",
    "        X2 = []\n",
    "        for i in range(X1.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                col = np.where(col_is_nan == True)[0]\n",
    "                for c in col:\n",
    "                    value = np.random.choice(candidate_mean[c])\n",
    "                    x = np.nan_to_num(x, nan=value)\n",
    "            \n",
    "            X2.extend(np.split(x, 7))\n",
    "\n",
    "        X = np.array(X2)\n",
    "\n",
    "        ## Final\n",
    "        X = X[:,:,:,newaxis] # For CNN, ResNet, ...\n",
    "        # X = X.reshape(X.shape[0], -1) # For DNN\n",
    "        # print(\"Expected True:\", np.all(np.isfinite(X))) # expected True\n",
    "        return X\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "# tmp = fe.transform(X_target)\n",
    "# print(tmp.shape)\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "def get_sample_weight(source, target):\n",
    "    target = pd.DataFrame(target)\n",
    "    source = pd.DataFrame(source)\n",
    "    target['is_z'] = 0 # 0 means test set\n",
    "    source['is_z'] = 1 # 1 means training set\n",
    "    source_and_target = pd.concat( [target, source], ignore_index=True, axis=0 )\n",
    "\n",
    "    labels = source_and_target['is_z'].values\n",
    "    source_and_target = source_and_target.drop('is_z', axis=1).values\n",
    "    target, source = target.values, source.values\n",
    "\n",
    "    print(\"source source:\", source.shape)\n",
    "    print(\"target target:\", target.shape)\n",
    "    print(\"combined source_and_target\", source_and_target.shape)\n",
    "\n",
    "    # Use a Random Forest Classifier to predict domain labels\n",
    "    # clf = RFC(n_estimators=50, max_depth=2, min_samples_leaf=150)\n",
    "    clf = RFC(n_estimators=10, max_depth=2,)\n",
    "\n",
    "    predictions = np.zeros(labels.shape)\n",
    "    skf = SKF(n_splits=10, shuffle=True,)\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(source_and_target, labels)):\n",
    "        print('Training discriminator model for fold {}'.format(fold))\n",
    "        X_train, X_test = source_and_target[train_idx], source_and_target[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "            \n",
    "        clf.fit(X_train, y_train)\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "        predictions[test_idx] = probs\n",
    "    print('ROC-AUC for target and source distributions:', AUC(labels, predictions))\n",
    "\n",
    "    # Calculate the sample weights\n",
    "    predictions_Z = predictions[len(target):]\n",
    "    weights = (1./predictions_Z) - 1. \n",
    "    weights /= np.mean(weights) # we do this to re-normalize the computed log-loss\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "source_dataset: <BatchDataset shapes: ((32, 96, 9, 1), (32, 1)), types: (tf.float64, tf.float32)> length: 10086\n",
      "da_dataset <_DirectedInterleaveDataset shapes: ((32, 96, 9, 1), (32, 1), (32, 96, 9, 1), (32, 1)), types: (tf.float64, tf.float32, tf.float64, tf.int64)> length: 285\n",
      "test_dataset <BatchDataset shapes: ((None, 96, 9, 1), (None, 1)), types: (tf.float64, tf.float32)>\n",
      "test_dataset_used <BatchDataset shapes: ((32, 96, 9, 1), (32, 1)), types: (tf.float64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "#CONSTANTS\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 1\n",
    "EPOCH = 5\n",
    "\n",
    "\n",
    "#Prepare Datasets\n",
    "y_source = np.repeat(y_source, 7)\n",
    "y_source = y_source.reshape(-1, 1)\n",
    "y_target = np.repeat(y_target, 7)\n",
    "y_target = y_target.reshape(-1, 1)\n",
    "y_test.target = np.repeat(y_test.target, 7)\n",
    "y_test.target = y_test.target.reshape(-1, 1)\n",
    "\n",
    "# weights = get_sample_weight(X_source.reshape(X_source.shape[0], -1),\n",
    "#                             X_target.reshape(X_target.shape[0], -1))\n",
    "# print(\"weights\", weights.shape)\n",
    "\n",
    "length = y_target.shape[0]\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((X_source, y_source,)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "tmp = tf.data.Dataset.from_tensor_slices((X_source[:length], y_source[:length], X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "tmp1 = tmp.map( lambda a, b, c, d: (tf.image.random_flip_left_right(a), b, tf.image.random_flip_left_right(c), d) )\n",
    "tmp2 = tmp.map( lambda a, b, c, d: (tf.image.random_flip_up_down(a), b, tf.image.random_flip_up_down(c), d) )\n",
    "da_dataset = tf.data.experimental.sample_from_datasets([tmp, tmp1, tmp2])\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target)).batch(len(y_test.target)) #Test Dataset over Target Domain\n",
    "test_dataset_used = tf.data.Dataset.from_tensor_slices((X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True) #Test Dataset over Target (used for training)\n",
    "\n",
    "# source_dataset = source_dataset.repeat()\n",
    "# da_dataset = da_dataset.repeat()\n",
    "# test_dataset = test_dataset.repeat()\n",
    "# test_dataset_used = test_dataset_used.repeat()\n",
    "\n",
    "print(\"source_dataset:\", source_dataset,\n",
    "      \"length:\", len(list(source_dataset.as_numpy_iterator())) )\n",
    "print(\"da_dataset\", da_dataset,\n",
    "      \"length:\", len(list(da_dataset.as_numpy_iterator())) )\n",
    "print(\"test_dataset\", test_dataset)\n",
    "print(\"test_dataset_used\", test_dataset_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before <BatchDataset shapes: ((32, 96, 9, 1), (32, 1)), types: (tf.float64, tf.float32)>\n",
      "<_DirectedInterleaveDataset shapes: ((32, 96, 9, 1), (32, 1)), types: (tf.float64, tf.float32)>\n",
      "after source_dataset: 10086\n",
      "after tmp_set: 10086\n"
     ]
    }
   ],
   "source": [
    "print(\"before\", source_dataset)\n",
    "tmp_set = source_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) )\n",
    "\n",
    "new_ds = tf.data.experimental.sample_from_datasets([source_dataset,tmp_set])\n",
    "print(new_ds)\n",
    "print(\"after source_dataset:\", len(list(source_dataset.as_numpy_iterator())))\n",
    "print(\"after tmp_set:\", len(list(tmp_set.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(124306, 96, 9, 1)\n",
      "(124306, 1)\n"
     ]
    }
   ],
   "source": [
    "a = list(test_dataset.as_numpy_iterator())\n",
    "print(len(a))\n",
    "print(a[0][0].shape)\n",
    "print(a[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "\n",
    "\n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Feature Extractor\n",
    "        self.feature_extractor_layer0 = Conv2D(32, 2, activation='relu')\n",
    "        self.feature_extractor_layer1 = BatchNormalization()\n",
    "        self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        self.feature_extractor_layer3 = Conv2D(64, 2, activation='relu')\n",
    "        self.feature_extractor_layer4 = Dropout(0.5)\n",
    "        self.feature_extractor_layer5 = BatchNormalization()\n",
    "        self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        #Label Predictor\n",
    "        self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(1, activation='sigmoid')\n",
    "        \n",
    "        #Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "    def call(self, x, train=False, source_train=False, lamda=1.0):\n",
    "        # print(\"x:\", x)\n",
    "\n",
    "        #Feature Extractor\n",
    "        # x = tf.keras.Input(shape=(672, 9, 1), name=\"Input_Layer\")\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        # x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        x = self.feature_extractor_layer3(x)\n",
    "        # x = self.feature_extractor_layer4(x, training=train)\n",
    "        # x = self.feature_extractor_layer5(x, training=train)\n",
    "        x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        print(\"x before feature:\", x)\n",
    "        feature = tf.reshape(x, [-1, 23 * 1 * 64])\n",
    "        # print(\"feature:\", feature)\n",
    "        \n",
    "        #Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        # print(\"feature_slice\", feature_slice)\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        # print(\"l_logits\", l_logits)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        if source_train is True:\n",
    "            print(\"! ONLY label predictor !\")\n",
    "            return l_logits\n",
    "        else:\n",
    "            print(\"! Domain predictor !\")\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            print(\"d_logits\", d_logits)\n",
    "            \n",
    "            return l_logits, d_logits\n",
    "\n",
    "\n",
    "model = DANN()\n",
    "\n",
    "\n",
    "def loss_func(input_logits, target_labels):\n",
    "    # print(\"\\tinput_logits:\", type(input_logits), input_logits.shape)\n",
    "    # print(\"\\ttarget_labels:\", type(target_labels), target_labels.shape)\n",
    "    # return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    # return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_pred=input_logits, y_true=target_labels))\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return loss_func(l_logits, labels)\n",
    "    else:\n",
    "        return loss_func(l_logits, labels) + loss_func(d_logits, domain)\n",
    "\n",
    "\n",
    "model_optimizer = tf.optimizers.Adam() # tf.optimizers.SGD()\n",
    "# model_optimizer = tf.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                DOMAIN ADAPTION\n",
      "\n",
      "============ EPOCH 0 ============\n",
      "lamda: 0.0\n",
      "\n",
      "---- train_step_da\n",
      "x before feature: Tensor(\"dann_1/max_pooling2d_3/MaxPool:0\", shape=(64, 23, 1, 64), dtype=float32)\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann_1/dense_9/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "\n",
      "---- train_step_da\n",
      "x before feature: Tensor(\"dann_1/max_pooling2d_3/MaxPool:0\", shape=(64, 23, 1, 64), dtype=float32)\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann_1/dense_9/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 0 :\t Source Accuracy : 81.726%  |  \n",
      "---- test_step\n",
      "x before feature: Tensor(\"dann_1/max_pooling2d_3/MaxPool:0\", shape=(124306, 23, 1, 64), dtype=float32)\n",
      "! ONLY label predictor !\n",
      "[Target] Metric: 84.774%  |  \n",
      "---- test_step\n",
      "x before feature: Tensor(\"dann_1/max_pooling2d_3/MaxPool:0\", shape=(32, 23, 1, 64), dtype=float32)\n",
      "! ONLY label predictor !\n",
      "[Target] Metric (used for training): 85.000%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "lamda: 0.46211717\n",
      "Training: Epoch 1 :\t Source Accuracy : 87.523%  |  [Target] Metric: 87.607%  |  [Target] Metric (used for training): 88.530%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "lamda: 0.7615942\n",
      "Training: Epoch 2 :\t Source Accuracy : 83.533%  |  [Target] Metric: 86.648%  |  [Target] Metric (used for training): 87.159%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "lamda: 0.90514827\n",
      "Training: Epoch 3 :\t Source Accuracy : 75.322%  |  [Target] Metric: 71.279%  |  [Target] Metric (used for training): 69.516%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 4 ============\n",
      "lamda: 0.9640276\n",
      "Training: Epoch 4 :\t Source Accuracy : 66.606%  |  [Target] Metric: 82.526%  |  [Target] Metric (used for training): 84.405%\n",
      "============ END EPOCH ============\n"
     ]
    }
   ],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.Precision()\n",
    "# epoch_accuracy = tf.keras.metrics.BinaryCrossentropy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    print(\"\\n---- train_step_source\")\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    print(\"\\n---- train_step_da\")\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    # print(\"\\timages\", images, images[:5])\n",
    "    # print(\"\\tlabels\", labels, labels[:5])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    print(\"\\n---- test_step\")\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy(output, labels)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    # print(\"                SOURCE ONLY\")\n",
    "    # for epoch in range(epochs):\n",
    "    #     print(\"\\n============ EPOCH {} ============\".format(epoch))\n",
    "    #     for batch in source_dataset:\n",
    "    #         # print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\n",
    "    #         train_step_source(*batch,)\n",
    "        \n",
    "    #     print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "    #     source_acc.append(epoch_accuracy.result())\n",
    "    #     test()\n",
    "    #     epoch_accuracy.reset_states()\n",
    "    #     print(\"============ END EPOCH ============\", end=\"\\n\")\n",
    "\n",
    "    print(\"                DOMAIN ADAPTION\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\n============ EPOCH {} ============\".format(epoch))\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-epochs * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')        \n",
    "        print(\"lamda:\", lamda)\n",
    "        # print(\"dataset\", dataset)\n",
    "        for batch in dataset:\n",
    "            # print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "        print(\"============ END EPOCH ============\", end=\"\\n\")\n",
    "\n",
    "\n",
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset_used:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"[Target] Metric (used for training): {:.3%}\".format(epoch_accuracy.result()))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "\n",
    "\n",
    "## 训练\n",
    "#train('source', 5)\n",
    "\n",
    "train('domain-adaptation', EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效果评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.target.shape: (124306, 96, 9, 1)\n",
      "x before feature: tf.Tensor(\n",
      "[[[[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.831 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.747 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.747 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.743 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.747 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.741 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.751 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.751 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.749 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.735 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.758 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.747 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.747 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.748 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.785 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.852 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.749 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.882 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 24.940 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 22.127 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 22.901 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 26.077 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 32.915 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.791 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.789 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.786 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.699 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.756 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.937 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 21.746 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.670 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.666 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.633 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.629 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.668 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.671 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.680 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.659 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.894 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.868 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.687 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.712 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.701 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.699 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.696 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.721 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.715 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.676 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.688 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.639 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.600 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.598 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.601 0.000 0.000 0.000]]]\n",
      "\n",
      "\n",
      " [[[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.556 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.527 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.528 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.528 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.528 0.000 0.000 0.000]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.509 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.529 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.527 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.533 0.000 0.000 0.000]]\n",
      "\n",
      "  [[0.000 0.000 0.000 0.000 0.000 ... 0.000 14.591 0.000 0.000 0.000]]]], shape=(124306, 23, 1, 64), dtype=float32)\n",
      "! ONLY label predictor !\n",
      "[Target] Metric: 82.526%  |  "
     ]
    }
   ],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "# X = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "# print(X.shape)\n",
    "\n",
    "\n",
    "#Testing Dataset (Target Domain)\n",
    "# for batch in test_dataset:\n",
    "    # test_step(*batch)\n",
    "    # print(batch[0].shape)\n",
    "output = model(X_test.target, train=False, source_train=True)\n",
    "epoch_accuracy(output, y_test.target)\n",
    "    \n",
    "print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "test_acc.append(epoch_accuracy.result())\n",
    "epoch_accuracy.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用华为RAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions 36888 and 258216 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cf76bf0030cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_is\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_is\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     trained_workflow = problem.workflow.train_submission(\n\u001b[0m\u001b[1;32m     22\u001b[0m         '.', X_train, y_train, train_is,)\n\u001b[1;32m     23\u001b[0m     \u001b[0mX_fold_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_is\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold)\u001b[0m\n\u001b[1;32m     77\u001b[0m             fe, X.target_bkg)\n\u001b[1;32m     78\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpticalLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold)\u001b[0m\n\u001b[1;32m     35\u001b[0m             sanitize=False)\n\u001b[1;32m     36\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         clf.fit(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_bkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             X.target_bkg, y.source, y.target)\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/submissions/haobai_dann/./classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg, y_source, y_target)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Generate TensorFlows dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msource_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \"\"\"\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3327\u001b[0m         self._tensors[0].get_shape()[0]))\n\u001b[1;32m   3328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0m\u001b[1;32m   3330\u001b[0m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0m\u001b[1;32m    289\u001b[0m                        (self, other))\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 36888 and 258216 are not compatible"
     ]
    }
   ],
   "source": [
    "import rampwf as rw\n",
    "import numpy as np\n",
    "from others import cd\n",
    "\n",
    "ap_bagged_test = []\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "with cd(\"~/Codes/HuaweiRAMP\"):\n",
    "    problem = rw.utils.assert_read_problem()\n",
    "    X_train, y_train = problem.get_train_data(show=False)\n",
    "    X_test, y_test = problem.get_test_data(show=False)\n",
    "# 导入评价函数\n",
    "ap    = problem.score_types[0]\n",
    "# 设置crossvalidation\n",
    "splits = problem.get_cv(X_train, y_train, n_splits=10) # 默认10\n",
    "# 开始实验\n",
    "ap_train, ap_valid, ap_test, = [], [], []\n",
    "y_test_preds = []\n",
    "for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "    trained_workflow = problem.workflow.train_submission(\n",
    "        '.', X_train, y_train, train_is,)\n",
    "    X_fold_train = X_train.slice(train_is)\n",
    "    X_fold_valid = X_train.slice(valid_is)\n",
    "    \n",
    "    y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "    y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "    y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "    ap_train.append( ap(y_train.slice(train_is).target, y_train_pred[:,1]) )\n",
    "    ap_valid.append( ap(y_train.slice(valid_is).target, y_valid_pred[:,1]) )\n",
    "    ap_test.append( ap(y_test.target, y_test_pred[:,1]) )\n",
    "    print('-------------------------------------')\n",
    "    print('training ap on fold {} = {:.3f}'.format(fold_i, ap_train[-1]))\n",
    "    print('validation ap on fold {} = {:.3f}'.format(fold_i, ap_valid[-1]))\n",
    "    print('test ap on fold {} = {:.3f}'.format(fold_i, ap_test[-1]))\n",
    "    \n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# 计算排名指标: bagged average precision on test dataset\n",
    "score = ap(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1])\n",
    "ap_bagged_test.append(score)\n",
    "del problem, X_train, y_train, X_test, y_test, ap, splits, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}