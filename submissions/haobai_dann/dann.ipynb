{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Domain Adversarial Neural Network in Tensorflow](https://github.com/sghoshjr/Domain-Adversarial-Neural-Network/blob/master/DANN.py)\n",
    "\n",
    "# 预处理\n",
    "## 装载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TRAIN SET ====\n",
      "  | X_source: (46110, 672, 9, 1) ; y_source: (46110,)\n",
      "A | X_source_bkg: (50862, 672, 9, 1)\n",
      "----\n",
      "  | X_target: (438, 672, 9, 1) ; y_target: (438,)\n",
      "B | X_target_bkg: (29592, 672, 9, 1)\n",
      "  | X_target_unlabeled: (8202, 672, 9, 1)\n",
      "==== TEST SET ====\n",
      "  | X_test.target: (17758, 672, 9, 1) ; y_test.target: (17758,)\n",
      "B | X_test.target_bkg: (47275, 672, 9, 1)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Deal with NaN and flatten the matrix to size (sample, 6720).\n",
    "        Executed on every input data (i.e., source, bkg, target) and passed\n",
    "        the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        #! ATTENTION\n",
    "        # The idea is supposed to eliminate the common columns filled entirely \n",
    "        # by NaN. But in this competition, since we don't have access to\n",
    "        # `OpticalDataset` object, it's impossible to communicate informations\n",
    "        # between datasets. So, here it deletes columns that are found on public\n",
    "        # dataset.\n",
    "        X = np.delete(X, [3,], axis=2)\n",
    "        X = X.astype(np.float64)\n",
    "        \n",
    "        ## 1st round\n",
    "        X1, nanmean = [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            nanmean.append(np.nanmean(x, axis=0))\n",
    "\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                X1.append(x) # deal later\n",
    "                continue\n",
    "            \n",
    "            # Rows with full Nan\n",
    "            # Unachievable. Cause we don't have access to manipulate on labels\n",
    "            # row_is_nan = np.all(indice, axis=1)\n",
    "            # if (row_is_nan == True).any():\n",
    "            #     row = np.where(row_is_nan == True)[0]\n",
    "            #     if len(row) >= x.shape[0]/4: # drop sample, /2=85%+, /4=75%+\n",
    "            #         continue\n",
    "            \n",
    "            # Columns with partial NaN\n",
    "            part_is_nan = np.any(indice, axis=0)\n",
    "            if (part_is_nan == True).any():\n",
    "                col = np.where(part_is_nan == True)[0]\n",
    "                # part_nan[i] = col[0]\n",
    "                for c in col:\n",
    "                    this = x[:,c]\n",
    "                    finite = this[np.isfinite(this)]\n",
    "                    fill = np.repeat(finite, np.ceil(len(this)/len(finite)))[:len(this)]\n",
    "                    x[:,c] = np.where(np.isfinite(this), this, fill)\n",
    "            \n",
    "            # Construct new array\n",
    "            X1.append(x)\n",
    "        X1, nanmean = np.array(X1), np.array(nanmean)\n",
    "\n",
    "        ## 2nd round\n",
    "        candidate_mean = []\n",
    "        for i in range(nanmean.shape[1]):\n",
    "            col = nanmean[i]\n",
    "            finite = col[np.isfinite(col)]\n",
    "            candidate_mean.append(finite)\n",
    "\n",
    "        X2 = []\n",
    "        for i in range(X1.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                col = np.where(col_is_nan == True)[0]\n",
    "                for c in col:\n",
    "                    value = np.random.choice(candidate_mean[c])\n",
    "                    x = np.nan_to_num(x, nan=value)\n",
    "            X2.append(x)\n",
    "        \n",
    "        X = np.array(X2)\n",
    "\n",
    "        ## Final\n",
    "        X = X[:,:,:,newaxis] # For CNN, ResNet, ...\n",
    "        # X = X.reshape(X.shape[0], -1) # For DNN\n",
    "        # print(\"Expected True:\", np.all(np.isfinite(X))) # expected True\n",
    "        return X\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "(438, 672, 9, 1)\n",
      "source_dataset: <BatchDataset shapes: ((32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.float32)>\n",
      "da_dataset <BatchDataset shapes: ((32, 672, 9, 1), (32, 1), (32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.float32, tf.float64, tf.int64)>\n",
      "test_dataset <BatchDataset shapes: ((None, 672, 9, 1), (None, 1)), types: (tf.float64, tf.float32)>\n",
      "test_dataset2 <BatchDataset shapes: ((32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "#CONSTANTS\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 1\n",
    "EPOCH = 5\n",
    "\n",
    "\n",
    "#Prepare Datasets\n",
    "y_source = y_source.reshape(-1, 1)\n",
    "y_target = y_target.reshape(-1, 1)\n",
    "y_test.target = y_test.target.reshape(-1, 1)\n",
    "\n",
    "print(X_target.shape)\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((X_source, y_source)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((X_source[:438], y_source[:438], X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target)).batch(len(y_test.target)) #Test Dataset over Target Domain\n",
    "test_dataset_used = tf.data.Dataset.from_tensor_slices((X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True) #Test Dataset over Target (used for training)\n",
    "\n",
    "# source_dataset = source_dataset.repeat()\n",
    "# da_dataset = da_dataset.repeat()\n",
    "# test_dataset = test_dataset.repeat()\n",
    "# test_dataset_used = test_dataset_used.repeat()\n",
    "\n",
    "print(\"source_dataset:\", source_dataset)\n",
    "print(\"da_dataset\", da_dataset)\n",
    "print(\"test_dataset\", test_dataset)\n",
    "print(\"test_dataset_used\", test_dataset_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(17758, 672, 9, 1)\n",
      "(17758, 1)\n"
     ]
    }
   ],
   "source": [
    "a = list(test_dataset.as_numpy_iterator())\n",
    "print(len(a))\n",
    "print(a[0][0].shape)\n",
    "print(a[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "\n",
    "\n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Feature Extractor\n",
    "        self.feature_extractor_layer0 = Conv2D(32, 2, activation='relu')\n",
    "        self.feature_extractor_layer1 = BatchNormalization()\n",
    "        self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        self.feature_extractor_layer3 = Conv2D(64, 2, activation='relu')\n",
    "        self.feature_extractor_layer4 = Dropout(0.5)\n",
    "        self.feature_extractor_layer5 = BatchNormalization()\n",
    "        self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        #Label Predictor\n",
    "        self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(1, activation=None)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "    def call(self, x, train=False, source_train=False, lamda=1.0):\n",
    "        # print(\"x:\", x)\n",
    "\n",
    "        #Feature Extractor\n",
    "        # x = tf.keras.Input(shape=(672, 9, 1), name=\"Input_Layer\")\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        # x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        x = self.feature_extractor_layer3(x)\n",
    "        # x = self.feature_extractor_layer4(x, training=train)\n",
    "        # x = self.feature_extractor_layer5(x, training=train)\n",
    "        x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        print(\"x before feature:\", x)\n",
    "        feature = tf.reshape(x, [-1, 167 * 1 * 64])\n",
    "        print(\"feature:\", feature)\n",
    "        \n",
    "        #Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        print(\"feature_slice\", feature_slice)\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        print(\"l_logits\", l_logits)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            print(\"d_logits\", d_logits)\n",
    "            \n",
    "            return l_logits, d_logits\n",
    "\n",
    "\n",
    "model = DANN()\n",
    "\n",
    "\n",
    "def loss_func(input_logits, target_labels):\n",
    "    # print(\"\\tinput_logits:\", type(input_logits), input_logits.shape)\n",
    "    # print(\"\\ttarget_labels:\", type(target_labels), target_labels.shape)\n",
    "    # return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    # return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_pred=input_logits, y_true=target_labels))\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return loss_func(l_logits, labels)\n",
    "    else:\n",
    "        return loss_func(l_logits, labels) + loss_func(d_logits, domain)\n",
    "\n",
    "\n",
    "model_optimizer = tf.optimizers.Adam() # tf.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ EPOCH 0 ============\n",
      "\n",
      "---- train_step_da\n",
      "x before feature: Tensor(\"dann/max_pooling2d_1/MaxPool:0\", shape=(64, 167, 1, 64), dtype=float32)\n",
      "feature: Tensor(\"dann/Reshape:0\", shape=(64, 10688), dtype=float32)\n",
      "feature_slice Tensor(\"dann/Slice:0\", shape=(32, 10688), dtype=float32)\n",
      "l_logits Tensor(\"dann/dense_2/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "\n",
      "---- train_step_da\n",
      "x before feature: Tensor(\"dann/max_pooling2d_1/MaxPool:0\", shape=(64, 167, 1, 64), dtype=float32)\n",
      "feature: Tensor(\"dann/Reshape:0\", shape=(64, 10688), dtype=float32)\n",
      "feature_slice Tensor(\"dann/Slice:0\", shape=(32, 10688), dtype=float32)\n",
      "l_logits Tensor(\"dann/dense_2/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 0 :\t Source Accuracy : 100.000%  |  \n",
      "---- test_step\n",
      "x before feature: Tensor(\"dann/max_pooling2d_1/MaxPool:0\", shape=(17758, 167, 1, 64), dtype=float32)\n",
      "feature: Tensor(\"dann/Reshape:0\", shape=(17758, 10688), dtype=float32)\n",
      "feature_slice Tensor(\"dann/Reshape:0\", shape=(17758, 10688), dtype=float32)\n",
      "l_logits Tensor(\"dann/dense_2/BiasAdd:0\", shape=(17758, 1), dtype=float32)\n",
      "[Target] Metric: 100.000%  |  \n",
      "---- test_step\n",
      "x before feature: Tensor(\"dann/max_pooling2d_1/MaxPool:0\", shape=(32, 167, 1, 64), dtype=float32)\n",
      "feature: Tensor(\"dann/Reshape:0\", shape=(32, 10688), dtype=float32)\n",
      "feature_slice Tensor(\"dann/Reshape:0\", shape=(32, 10688), dtype=float32)\n",
      "l_logits Tensor(\"dann/dense_2/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "[Target] Metric (used for training): 100.000%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "Training: Epoch 1 :\t Source Accuracy : 100.000%  |  [Target] Metric: 100.000%  |  [Target] Metric (used for training): 100.000%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "Training: Epoch 2 :\t Source Accuracy : 100.000%  |  [Target] Metric: 100.000%  |  [Target] Metric (used for training): 100.000%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "Training: Epoch 3 :\t Source Accuracy : 100.000%  |  [Target] Metric: 100.000%  |  [Target] Metric (used for training): 100.000%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 4 ============\n",
      "Training: Epoch 4 :\t Source Accuracy : 100.000%  |  [Target] Metric: 100.000%  |  [Target] Metric (used for training): 100.000%\n",
      "============ END EPOCH ============\n"
     ]
    }
   ],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.Precision()\n",
    "# epoch_accuracy = tf.keras.metrics.BinaryCrossentropy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    pass\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    print(\"\\n---- train_step_da\")\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    # print(\"\\timages\", images, images[:5])\n",
    "    # print(\"\\tlabels\", labels, labels[:5])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    print(\"\\n---- test_step\")\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy(output, labels)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\n============ EPOCH {} ============\".format(epoch))\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        # print(\"dataset\", dataset)\n",
    "        for batch in dataset:\n",
    "            # print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "        print(\"============ END EPOCH ============\", end=\"\\n\")\n",
    "\n",
    "\n",
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset_used:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"[Target] Metric (used for training): {:.3%}\".format(epoch_accuracy.result()))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "\n",
    "\n",
    "## 训练\n",
    "#train('source', 5)\n",
    "\n",
    "train('domain-adaptation', EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效果评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.target.shape: (17758, 672, 9, 1)\n",
      "[Target] Metric: 100.000%  |  "
     ]
    }
   ],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "# X = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "# print(X.shape)\n",
    "\n",
    "\n",
    "#Testing Dataset (Target Domain)\n",
    "for batch in test_dataset:\n",
    "    test_step(*batch)\n",
    "    \n",
    "print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "test_acc.append(epoch_accuracy.result())\n",
    "epoch_accuracy.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}