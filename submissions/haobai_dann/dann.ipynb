{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Domain Adversarial Neural Network in Tensorflow](https://github.com/sghoshjr/Domain-Adversarial-Neural-Network/blob/master/DANN.py)\n",
    "\n",
    "# 预处理\n",
    "## 装载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TRAIN SET ====\n",
      "  | X_source: (46110, 672, 9, 1) ; y_source: (46110,)\n",
      "A | X_source_bkg: (50862, 672, 9, 1)\n",
      "----\n",
      "  | X_target: (438, 672, 9, 1) ; y_target: (438,)\n",
      "B | X_target_bkg: (29592, 672, 9, 1)\n",
      "  | X_target_unlabeled: (8202, 672, 9, 1)\n",
      "==== TEST SET ====\n",
      "  | X_test.target: (17758, 672, 9, 1) ; y_test.target: (17758,)\n",
      "B | X_test.target_bkg: (47275, 672, 9, 1)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "# from numpy import newaxis\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# class FeatureExtractor:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         ''' Deal with NaN and flatten the matrix to size (sample, 6720).\n",
    "#         Executed on every input data (i.e., source, bkg, target) and passed\n",
    "#         the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         `X`: ndarray of (sample, 672, 10)\n",
    "#             3D input dataset(sample, time, features)\n",
    "        \n",
    "#         Returns\n",
    "#         -------\n",
    "#         `X`: ndarray of (sample, 6720)\n",
    "#             The filtered dataset\n",
    "#         '''\n",
    "#         #! ATTENTION\n",
    "#         # The idea is supposed to eliminate the common columns filled entirely \n",
    "#         # by NaN. But in this competition, since we don't have access to\n",
    "#         # `OpticalDataset` object, it's impossible to communicate informations\n",
    "#         # between datasets. So, here it deletes columns that are found on public\n",
    "#         # dataset.\n",
    "#         X = np.delete(X, [3,], axis=2)\n",
    "#         X = X.astype(np.float64)\n",
    "        \n",
    "#         ## 1st round\n",
    "#         X1, nanmean = [], []\n",
    "#         for i in range(X.shape[0]):\n",
    "#             x = X[i]\n",
    "#             indice = ~np.isfinite(x)\n",
    "#             nanmean.append(np.nanmean(x, axis=0))\n",
    "\n",
    "#             # Columns with full Nan\n",
    "#             col_is_nan = np.all(indice, axis=0)\n",
    "#             if (col_is_nan == True).any():\n",
    "#                 X1.append(x) # deal later\n",
    "#                 continue\n",
    "            \n",
    "#             # Rows with full Nan\n",
    "#             # Unachievable. Cause we don't have access to manipulate on labels\n",
    "#             # row_is_nan = np.all(indice, axis=1)\n",
    "#             # if (row_is_nan == True).any():\n",
    "#             #     row = np.where(row_is_nan == True)[0]\n",
    "#             #     if len(row) >= x.shape[0]/4: # drop sample, /2=85%+, /4=75%+\n",
    "#             #         continue\n",
    "            \n",
    "#             # Columns with partial NaN\n",
    "#             part_is_nan = np.any(indice, axis=0)\n",
    "#             if (part_is_nan == True).any():\n",
    "#                 col = np.where(part_is_nan == True)[0]\n",
    "#                 # part_nan[i] = col[0]\n",
    "#                 for c in col:\n",
    "#                     this = x[:,c]\n",
    "#                     finite = this[np.isfinite(this)]\n",
    "#                     fill = np.repeat(finite, np.ceil(len(this)/len(finite)))[:len(this)]\n",
    "#                     x[:,c] = np.where(np.isfinite(this), this, fill)\n",
    "            \n",
    "#             # Construct new array\n",
    "#             X1.append(x)\n",
    "#         X1, nanmean = np.array(X1), np.array(nanmean)\n",
    "\n",
    "#         ## 2nd round\n",
    "#         candidate_mean = []\n",
    "#         for i in range(nanmean.shape[1]):\n",
    "#             col = nanmean[i]\n",
    "#             finite = col[np.isfinite(col)]\n",
    "#             candidate_mean.append(finite)\n",
    "\n",
    "#         X2 = []\n",
    "#         for i in range(X1.shape[0]):\n",
    "#             x = X[i]\n",
    "#             indice = ~np.isfinite(x)\n",
    "#             # Columns with full Nan\n",
    "#             col_is_nan = np.all(indice, axis=0)\n",
    "#             if (col_is_nan == True).any():\n",
    "#                 col = np.where(col_is_nan == True)[0]\n",
    "#                 for c in col:\n",
    "#                     value = np.random.choice(candidate_mean[c])\n",
    "#                     x = np.nan_to_num(x, nan=value)\n",
    "#             X2.append(x)\n",
    "        \n",
    "#         X = np.array(X2)\n",
    "\n",
    "#         ## Final\n",
    "#         X = X[:,:,:,newaxis] # For CNN, ResNet, ...\n",
    "#         # X = X.reshape(X.shape[0], -1) # For DNN\n",
    "#         # print(\"Expected True:\", np.all(np.isfinite(X))) # expected True\n",
    "#         return X\n",
    "\n",
    "from feature_extractor import FeatureExtractor\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "(438, 672, 9, 1)\n",
      "source_dataset: <BatchDataset shapes: ((32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.float32)> length: 1440\n",
      "da_dataset <BatchDataset shapes: ((32, 672, 9, 1), (32, 1), (32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.float32, tf.float64, tf.int64)> length: 13\n",
      "test_dataset <BatchDataset shapes: ((None, 672, 9, 1), (None, 1)), types: (tf.float64, tf.float32)>\n",
      "test_dataset_used <BatchDataset shapes: ((32, 672, 9, 1), (32, 1)), types: (tf.float64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "#CONSTANTS\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 1\n",
    "EPOCH = 5\n",
    "\n",
    "\n",
    "#Prepare Datasets\n",
    "y_source = y_source.reshape(-1, 1)\n",
    "y_target = y_target.reshape(-1, 1)\n",
    "y_test.target = y_test.target.reshape(-1, 1)\n",
    "\n",
    "print(X_target.shape)\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((X_source, y_source)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((X_source[:438], y_source[:438], X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target)).batch(len(y_test.target)) #Test Dataset over Target Domain\n",
    "test_dataset_used = tf.data.Dataset.from_tensor_slices((X_target, y_target)).shuffle(100).batch(BATCH_SIZE, drop_remainder=True) #Test Dataset over Target (used for training)\n",
    "\n",
    "# source_dataset = source_dataset.repeat()\n",
    "# da_dataset = da_dataset.repeat()\n",
    "# test_dataset = test_dataset.repeat()\n",
    "# test_dataset_used = test_dataset_used.repeat()\n",
    "\n",
    "print(\"source_dataset:\", source_dataset,\n",
    "      \"length:\", len(list(source_dataset.as_numpy_iterator())) )\n",
    "print(\"da_dataset\", da_dataset,\n",
    "      \"length:\", len(list(da_dataset.as_numpy_iterator())) )\n",
    "print(\"test_dataset\", test_dataset)\n",
    "print(\"test_dataset_used\", test_dataset_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17758, 672, 9, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(17758, 672, 9, 1)\n",
      "(17758, 1)\n"
     ]
    }
   ],
   "source": [
    "a = list(test_dataset.as_numpy_iterator())\n",
    "print(len(a))\n",
    "print(a[0][0].shape)\n",
    "print(a[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Reversal Layer\n",
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "\n",
    "\n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Feature Extractor\n",
    "        self.feature_extractor_layer0 = Conv2D(32, 2, activation='relu')\n",
    "        self.feature_extractor_layer1 = BatchNormalization()\n",
    "        self.feature_extractor_layer2 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        self.feature_extractor_layer3 = Conv2D(64, 2, activation='relu')\n",
    "        self.feature_extractor_layer4 = Dropout(0.5)\n",
    "        self.feature_extractor_layer5 = BatchNormalization()\n",
    "        self.feature_extractor_layer6 = MaxPool2D(pool_size=(2, 2),)\n",
    "        \n",
    "        #Label Predictor\n",
    "        self.label_predictor_layer0 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.label_predictor_layer2 = Dense(1, activation='sigmoid')\n",
    "        \n",
    "        #Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(100, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2, activation=None)\n",
    "        \n",
    "    def call(self, x, train=False, source_train=False, lamda=1.0):\n",
    "        # print(\"x:\", x)\n",
    "\n",
    "        #Feature Extractor\n",
    "        # x = tf.keras.Input(shape=(672, 9, 1), name=\"Input_Layer\")\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        # x = self.feature_extractor_layer1(x, training=train)\n",
    "        x = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        x = self.feature_extractor_layer3(x)\n",
    "        # x = self.feature_extractor_layer4(x, training=train)\n",
    "        # x = self.feature_extractor_layer5(x, training=train)\n",
    "        x = self.feature_extractor_layer6(x)\n",
    "        \n",
    "        # print(\"x before feature:\", x)\n",
    "        feature = tf.reshape(x, [-1, 167 * 1 * 64])\n",
    "        # print(\"feature:\", feature)\n",
    "        \n",
    "        #Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "        # print(\"feature_slice\", feature_slice)\n",
    "        \n",
    "        lp_x = self.label_predictor_layer0(feature_slice)\n",
    "        lp_x = self.label_predictor_layer1(lp_x)\n",
    "        l_logits = self.label_predictor_layer2(lp_x)\n",
    "        # print(\"l_logits\", l_logits)\n",
    "        \n",
    "        #Domain Predictor\n",
    "        if source_train is True:\n",
    "            print(\"! ONLY label predictor !\")\n",
    "            return l_logits\n",
    "        else:\n",
    "            print(\"! Domain predictor !\")\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda)    #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            print(\"d_logits\", d_logits)\n",
    "            \n",
    "            return l_logits, d_logits\n",
    "\n",
    "\n",
    "model = DANN()\n",
    "\n",
    "\n",
    "def loss_func(input_logits, target_labels):\n",
    "    # print(\"\\tinput_logits:\", type(input_logits), input_logits.shape)\n",
    "    # print(\"\\ttarget_labels:\", type(target_labels), target_labels.shape)\n",
    "    # return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    # return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_pred=input_logits, y_true=target_labels))\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return loss_func(l_logits, labels)\n",
    "    else:\n",
    "        return loss_func(l_logits, labels) + loss_func(d_logits, domain)\n",
    "\n",
    "\n",
    "model_optimizer = tf.optimizers.Adam() # tf.optimizers.SGD()\n",
    "# model_optimizer = tf.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                DOMAIN ADAPTION\n",
      "\n",
      "============ EPOCH 0 ============\n",
      "lamda: 0.0\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 0 :\t Source Accuracy : 48.980%  |  \n",
      "---- test_step\n",
      "! ONLY label predictor !\n",
      "[Target] Metric: 44.202%  |  \n",
      "---- test_step\n",
      "! ONLY label predictor !\n",
      "[Target] Metric (used for training): 38.095%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "lamda: 0.2\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 1 :\t Source Accuracy : 71.429%  |  [Target] Metric: 84.045%  |  [Target] Metric (used for training): 76.190%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "lamda: 0.4\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 2 :\t Source Accuracy : 76.471%  |  [Target] Metric: 60.157%  |  [Target] Metric (used for training): 55.814%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "lamda: 0.6\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 3 :\t Source Accuracy : 77.551%  |  [Target] Metric: 70.793%  |  [Target] Metric (used for training): 70.115%\n",
      "============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 4 ============\n",
      "lamda: 0.8\n",
      "\n",
      "---- train_step_da\n",
      "! Domain predictor !\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(64, 2), dtype=float32)\n",
      "Training: Epoch 4 :\t Source Accuracy : 91.667%  |  [Target] Metric: 63.165%  |  [Target] Metric (used for training): 60.714%\n",
      "============ END EPOCH ============\n"
     ]
    }
   ],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')\n",
    "\n",
    "\n",
    "epoch_accuracy = tf.keras.metrics.Precision()\n",
    "# epoch_accuracy = tf.keras.metrics.BinaryCrossentropy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    print(\"\\n---- train_step_source\")\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_accuracy(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    print(\"\\n---- train_step_da\")\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    labels = s_labels\n",
    "    # print(\"\\timages\", images, images[:5])\n",
    "    # print(\"\\tlabels\", labels, labels[:5])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output[0], labels, output[1], domain_labels)\n",
    "        epoch_accuracy(output[0], labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    print(\"\\n---- test_step\")\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy(output, labels)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    # print(\"                SOURCE ONLY\")\n",
    "    # for epoch in range(epochs):\n",
    "    #     print(\"\\n============ EPOCH {} ============\".format(epoch))\n",
    "    #     for batch in source_dataset:\n",
    "    #         # print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\n",
    "    #         train_step_source(*batch,)\n",
    "        \n",
    "    #     print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "    #     source_acc.append(epoch_accuracy.result())\n",
    "    #     test()\n",
    "    #     epoch_accuracy.reset_states()\n",
    "    #     print(\"============ END EPOCH ============\", end=\"\\n\")\n",
    "\n",
    "    print(\"                DOMAIN ADAPTION\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\n============ EPOCH {} ============\".format(epoch))\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "        \n",
    "        lamda = epoch / epochs\n",
    "        print(\"lamda:\", lamda)\n",
    "        # print(\"dataset\", dataset)\n",
    "        for batch in dataset:\n",
    "            # print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy.result()), end='  |  ')\n",
    "        acc_list.append(epoch_accuracy.result())\n",
    "        test()\n",
    "        epoch_accuracy.reset_states()\n",
    "        print(\"============ END EPOCH ============\", end=\"\\n\")\n",
    "\n",
    "\n",
    "def test():\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset_used:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"[Target] Metric (used for training): {:.3%}\".format(epoch_accuracy.result()))\n",
    "    test2_acc.append(epoch_accuracy.result())\n",
    "    epoch_accuracy.reset_states()\n",
    "\n",
    "\n",
    "## 训练\n",
    "#train('source', 5)\n",
    "\n",
    "train('domain-adaptation', EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效果评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.target.shape: (17758, 672, 9, 1)\n",
      "! ONLY label predictor !\n",
      "[Target] Metric: 63.165%  |  "
     ]
    }
   ],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "# X = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "# print(X.shape)\n",
    "\n",
    "\n",
    "#Testing Dataset (Target Domain)\n",
    "# for batch in test_dataset:\n",
    "    # test_step(*batch)\n",
    "    # print(batch[0].shape)\n",
    "output = model(X_test.target, train=False, source_train=True)\n",
    "epoch_accuracy(output, y_test.target)\n",
    "    \n",
    "print(\"[Target] Metric: {:.3%}\".format(epoch_accuracy.result()), end='  |  ')\n",
    "test_acc.append(epoch_accuracy.result())\n",
    "epoch_accuracy.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用华为RAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n",
      "---- Prepare Datasets\n",
      "Length of y_target: 20\n",
      "source_dataset: <BatchDataset shapes: ((100, 672, 9, 1), (100, 1)), types: (tf.float64, tf.float32)> length: 368\n",
      "da_dataset <BatchDataset shapes: ((10, 672, 9, 1), (10, 1), (10, 672, 9, 1), (10, 1)), types: (tf.float64, tf.float32, tf.float64, tf.int64)> length: 2\n",
      "---- End ----\n",
      "                SOURCE ONLY\n",
      "\n",
      "============ EPOCH 0 ============\n",
      "lamda: 0.4\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 0 :\t Source Accuracy : 80.342%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "lamda: 0.6000000000000001\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 1 :\t Source Accuracy : 89.225%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "lamda: 0.8\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 2 :\t Source Accuracy : 92.378%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "lamda: 1.0\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 3 :\t Source Accuracy : 93.088%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 4 ============\n",
      "lamda: 1.2000000000000002\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 4 :\t Source Accuracy : 95.375%  |  ============ END EPOCH ============\n",
      "                DOMAIN ADAPTION\n",
      "\n",
      "============ EPOCH 0 ============\n",
      "lamda: 0.4\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "Training: Epoch 0 :\t Source Accuracy : 100.000%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "lamda: 0.6000000000000001\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "Training: Epoch 1 :\t Source Accuracy : 100.000%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "lamda: 0.8\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "Training: Epoch 2 :\t Source Accuracy : 100.000%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "lamda: 1.0\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "Training: Epoch 3 :\t Source Accuracy : 100.000%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 4 ============\n",
      "lamda: 1.2000000000000002\n",
      "\n",
      "---- train_step_da\n",
      "d_logits Tensor(\"dann/dense_4/BiasAdd:0\", shape=(20, 2), dtype=float32)\n",
      "\toutput: <class 'tuple'> 2\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice:0\", shape=(5, 1), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"strided_slice_1:0\", shape=(5, 2), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_step_da at 0x7fd0cbfd7af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training: Epoch 4 :\t Source Accuracy : 100.000%  |  ============ END EPOCH ============\n",
      "-------------------------------------\n",
      "training ap on fold 0 = 0.392\n",
      "validation ap on fold 0 = 0.260\n",
      "test ap on fold 0 = 0.189\n",
      "---- Prepare Datasets\n",
      "Length of y_target: 20\n",
      "source_dataset: <BatchDataset shapes: ((100, 672, 9, 1), (100, 1)), types: (tf.float64, tf.float32)> length: 368\n",
      "da_dataset <BatchDataset shapes: ((10, 672, 9, 1), (10, 1), (10, 672, 9, 1), (10, 1)), types: (tf.float64, tf.float32, tf.float64, tf.int64)> length: 2\n",
      "---- End ----\n",
      "                SOURCE ONLY\n",
      "\n",
      "============ EPOCH 0 ============\n",
      "lamda: 0.4\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 0 :\t Source Accuracy : 83.583%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 1 ============\n",
      "lamda: 0.6000000000000001\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 1 :\t Source Accuracy : 90.045%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 2 ============\n",
      "lamda: 0.8\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n",
      "Training: Epoch 2 :\t Source Accuracy : 92.829%  |  ============ END EPOCH ============\n",
      "\n",
      "============ EPOCH 3 ============\n",
      "lamda: 1.0\n",
      "\n",
      "---- train_step_source\n",
      "\toutput: <class 'tensorflow.python.framework.ops.Tensor'> (100, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf76bf0030cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_is\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_is\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     trained_workflow = problem.workflow.train_submission(\n\u001b[0m\u001b[1;32m     22\u001b[0m         '.', X_train, y_train, train_is,)\n\u001b[1;32m     23\u001b[0m     \u001b[0mX_fold_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_is\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold)\u001b[0m\n\u001b[1;32m     77\u001b[0m             fe, X.target_bkg)\n\u001b[1;32m     78\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpticalLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_workflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/external_imports/utils/workflow.py\u001b[0m in \u001b[0;36mtrain_submission\u001b[0;34m(self, module_path, X, y, fold)\u001b[0m\n\u001b[1;32m     35\u001b[0m             sanitize=False)\n\u001b[1;32m     36\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         clf.fit(\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_bkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             X.target_bkg, y.source, y.target)\n",
      "\u001b[0;32m~/Codes/HuaweiRAMP/submissions/haobai_dann/./classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg, y_source, y_target)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# print(\"\\tbatch length:\", len(batch), \"batch[0]:\", batch[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 train_step_source(*batch,\n\u001b[0m\u001b[1;32m     99\u001b[0m                     \u001b[0mlamda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlamda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import rampwf as rw\n",
    "import numpy as np\n",
    "from others import cd\n",
    "\n",
    "ap_bagged_test = []\n",
    "\n",
    "\n",
    "# 载入数据\n",
    "with cd(\"~/Codes/HuaweiRAMP\"):\n",
    "    problem = rw.utils.assert_read_problem()\n",
    "    X_train, y_train = problem.get_train_data(show=False)\n",
    "    X_test, y_test = problem.get_test_data(show=False)\n",
    "# 导入评价函数\n",
    "ap    = problem.score_types[0]\n",
    "# 设置crossvalidation\n",
    "splits = problem.get_cv(X_train, y_train, n_splits=10) # 默认10\n",
    "# 开始实验\n",
    "ap_train, ap_valid, ap_test, = [], [], []\n",
    "y_test_preds = []\n",
    "for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "    trained_workflow = problem.workflow.train_submission(\n",
    "        '.', X_train, y_train, train_is,)\n",
    "    X_fold_train = X_train.slice(train_is)\n",
    "    X_fold_valid = X_train.slice(valid_is)\n",
    "    \n",
    "    y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "    y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "    y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "    ap_train.append( ap(y_train.slice(train_is).target, y_train_pred[:,1]) )\n",
    "    ap_valid.append( ap(y_train.slice(valid_is).target, y_valid_pred[:,1]) )\n",
    "    ap_test.append( ap(y_test.target, y_test_pred[:,1]) )\n",
    "    print('-------------------------------------')\n",
    "    print('training ap on fold {} = {:.3f}'.format(fold_i, ap_train[-1]))\n",
    "    print('validation ap on fold {} = {:.3f}'.format(fold_i, ap_valid[-1]))\n",
    "    print('test ap on fold {} = {:.3f}'.format(fold_i, ap_test[-1]))\n",
    "    \n",
    "    y_test_preds.append(y_test_pred)\n",
    "\n",
    "# 计算排名指标: bagged average precision on test dataset\n",
    "score = ap(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1])\n",
    "ap_bagged_test.append(score)\n",
    "del problem, X_train, y_train, X_test, y_test, ap, splits, y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}