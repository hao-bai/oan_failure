{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集概览\n",
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集\n",
    "### City A (source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"source labeled (City A weak & failure):\", X_train.source.shape)\n",
    "# print(np.where(np.isnan(X_train.source[0])))\n",
    "print(\"|- labels:\", y_train.source.shape)\n",
    "print(\" |- weak=0:\", y_train.source[y_train.source==0].shape)\n",
    "print(\" |- failure=1:\", y_train.source[y_train.source==1].shape)\n",
    "print(\"source background (City A good):\", X_train.source_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target labeled (City B weak & failure): (438, 672, 10)\n",
      "|- labels: (438,)\n",
      " |- weak=0: (349,)\n",
      " |- failure=1: (89,)\n",
      "target background (City B good): (29592, 672, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_train.target.shape)\n",
    "print(\"|- labels:\", y_train.target.shape)\n",
    "print(\" |- weak=0:\", y_train.target[y_train.target==0].shape)\n",
    "print(\" |- failure=1:\", y_train.target[y_train.target==1].shape)\n",
    "print(\"target background (City B good):\", X_train.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集\n",
    "### City B (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target labeled (City B weak & failure): (17758, 672, 9, 1)\n",
      "|- labels: (17758,)\n",
      " |- weak=0: (15464,)\n",
      " |- failure=1: (2294,)\n",
      "target background (City B good): (47275, 672, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"target labeled (City B weak & failure):\", X_test.target.shape)\n",
    "print(\"|- labels:\", y_test.target.shape)\n",
    "print(\" |- weak=0:\", y_test.target[y_test.target==0].shape)\n",
    "print(\" |- failure=1:\", y_test.target[y_test.target==1].shape)\n",
    "print(\"target background (City B good):\", X_test.target_bkg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network\n",
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TRAIN SET ====\n",
      "  | X_source: (46110, 672, 9, 1) ; y_source: (46110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbai/Codes/HuaweiRAMP/submissions/haobai_dnn/feature_extractor.py:52: RuntimeWarning: Mean of empty slice\n",
      "  nanmean.append(np.nanmean(x, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A | X_source_bkg: (50862, 672, 9, 1)\n",
      "----\n",
      "  | X_target: (438, 672, 9, 1) ; y_target: (438,)\n",
      "B | X_target_bkg: (29592, 672, 9, 1)\n",
      "  | X_target_unlabeled: (8202, 672, 9, 1)\n",
      "==== TEST SET ====\n",
      "  | X_test.target: (17758, 672, 9, 1) ; y_test.target: (17758,)\n",
      "B | X_test.target_bkg: (47275, 672, 9, 1)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from feature_extractor import FeatureExtractor\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除NaN\n",
    "from numpy import newaxis\n",
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Replace NaN by 0 and flatten the matrix to size (sample, 6720).\n",
    "        Executed on every input data (i.e., source, bkg, target) and passed\n",
    "        the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        np.nan_to_num(X, copy=False)\n",
    "        return X[:,:,:,newaxis]\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "# 重命名\n",
    "from copy import deepcopy\n",
    "# 训练集\n",
    "print(\"==== TRAIN SET ====\")\n",
    "X_source = deepcopy( fe.transform(X_train.source) )\n",
    "print(\"  | X_source:\", X_source.shape, end=\" ; \")\n",
    "y_source = deepcopy( y_train.source )\n",
    "print(\"y_source:\", y_source.shape)\n",
    "X_source_bkg = deepcopy( fe.transform(X_train.source_bkg) )\n",
    "print(\"A | X_source_bkg:\", X_source_bkg.shape)\n",
    "X_target = deepcopy( fe.transform(X_train.target) )\n",
    "print(\"----\")\n",
    "print(\"  | X_target:\", X_target.shape, end=\" ; \")\n",
    "y_target = deepcopy( y_train.target )\n",
    "print(\"y_target:\", y_target.shape)\n",
    "X_target_bkg= deepcopy( fe.transform(X_train.target_bkg) )\n",
    "print(\"B | X_target_bkg:\", X_target_bkg.shape)\n",
    "X_target_unlabeled = deepcopy( fe.transform(X_train.target_unlabeled) )\n",
    "print(\"  | X_target_unlabeled:\", X_target_unlabeled.shape)\n",
    "# 测试集\n",
    "print(\"==== TEST SET ====\")\n",
    "X_test.target = fe.transform(X_test.target)\n",
    "print(\"  | X_test.target:\", X_test.target.shape, end=\" ; \")\n",
    "print(\"y_test.target:\", y_test.target.shape)\n",
    "X_test.target_bkg = fe.transform(X_test.target_bkg)\n",
    "print(\"B | X_test.target_bkg:\", X_test.target_bkg.shape)\n",
    "print(\"  | X_test.target_unlabeled:\", X_test.target_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集转换为TensorFlow格式\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_source, y_source)).batch(16)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_target, y_target)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target))\n",
    "\n",
    "# 额外操作\n",
    "train_dataset = train_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) )\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络模型\n",
    "参考资料\n",
    "\n",
    "1. [Introduction to ResNet in TensorFlow 2](https://adventuresinmachinelearning.com/introduction-resnet-tensorflow-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(filters, conv_size, input_data):\n",
    "    ''' A residual block of 3 layers\n",
    "    '''\n",
    "    # 1st layer with batch normalization\n",
    "    x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 2nd layer with batch normalization, but no activation function\n",
    "    x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # 3rd layer is residual addition with an activation function\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Layer (InputLayer)        [(None, 672, 9, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Layer01 (Conv2D)                (None, 671, 8, 32)   160         Input_Layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Layer02 (Conv2D)                (None, 670, 7, 64)   8256        Layer01[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer03 (MaxPooling2D)          (None, 335, 3, 64)   0           Layer02[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 335, 3, 64)   16448       Layer03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 335, 3, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 335, 3, 64)   16448       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 335, 3, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 335, 3, 64)   0           batch_normalization_1[0][0]      \n",
      "                                                                 Layer03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 335, 3, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 335, 3, 64)   16448       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 335, 3, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 335, 3, 64)   16448       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 335, 3, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 335, 3, 64)   0           batch_normalization_3[0][0]      \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 335, 3, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 335, 3, 64)   16448       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 335, 3, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 335, 3, 64)   16448       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 335, 3, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 335, 3, 64)   0           batch_normalization_5[0][0]      \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 335, 3, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 335, 3, 64)   16448       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 335, 3, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 335, 3, 64)   16448       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 335, 3, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 335, 3, 64)   0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 335, 3, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 335, 3, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 335, 3, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 335, 3, 64)   16448       batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 335, 3, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 335, 3, 64)   0           batch_normalization_9[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 335, 3, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 335, 3, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 335, 3, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 335, 3, 64)   16448       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 335, 3, 64)   256         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 335, 3, 64)   0           batch_normalization_11[0][0]     \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 335, 3, 64)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 335, 3, 64)   16448       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 335, 3, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 335, 3, 64)   16448       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 335, 3, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 335, 3, 64)   0           batch_normalization_13[0][0]     \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 335, 3, 64)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 335, 3, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 335, 3, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 335, 3, 64)   16448       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 335, 3, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 335, 3, 64)   0           batch_normalization_15[0][0]     \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 335, 3, 64)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 335, 3, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 335, 3, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 335, 3, 64)   16448       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 335, 3, 64)   256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 335, 3, 64)   0           batch_normalization_17[0][0]     \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 335, 3, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 335, 3, 64)   16448       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 335, 3, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 335, 3, 64)   16448       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 335, 3, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 335, 3, 64)   0           batch_normalization_19[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 335, 3, 64)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Layer-4 (Conv2D)                (None, 334, 2, 64)   16448       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Layer-3 (GlobalAveragePooling2D (None, 64)           0           Layer-4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer-2 (Dense)                 (None, 256)          16640       Layer-3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Layer-1 (Dropout)               (None, 256)          0           Layer-2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output_Layer (Dense)            (None, 1)            257         Layer-1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 375,841\n",
      "Trainable params: 373,281\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Layer to be used as an entry point into a Network (a graph of layers).\n",
    "# https://keras.io/api/layers/core_layers/input/\n",
    "inputs = tf.keras.Input(shape=(672, 9, 1), name=\"Input_Layer\")\n",
    "# inputs = layers.InputLayer(input_shape=(672, 10), name=\"Input_Layer\")\n",
    "\n",
    "# 2D convolution layer (e.g. spatial convolution over images).\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "x = layers.Conv2D(32, 2, activation='relu', name=\"Layer01\")(inputs)\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer02\")(x)\n",
    "\n",
    "# Max pooling operation for 2D spatial data.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "# pool_size: integer or tuple of 2 integers, window size over which to take the maximum.\n",
    "x = layers.MaxPooling2D(2, name=\"Layer03\")(x)\n",
    "\n",
    "num_res_net_blocks = 10 # 10个ResNet blocks\n",
    "for i in range(num_res_net_blocks):\n",
    "    x = res_net_block(64, 2, x)\n",
    "\n",
    "# [Final layers] a standard CNN layer\n",
    "x = layers.Conv2D(64, 2, activation='relu', name=\"Layer-4\")(x)\n",
    "# [Final layers] GAP layer\n",
    "x = layers.GlobalAveragePooling2D(name=\"Layer-3\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "# Just your regular densely-connected NN layer.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "# units: Positive integer, dimensionality of the output space.\n",
    "x = layers.Dense(256, activation='relu', name=\"Layer-2\")(x)\n",
    "# [Final layers] dropout layer\n",
    "x = layers.Dropout(0.5, name=\"Layer-1\")(x)\n",
    "# [Final layers] dense classification layers\n",
    "outputs = layers.Dense(1, activation='softmax', name=\"Output_Layer\")(x)\n",
    "res_net_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "res_net_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "'binary_crossentropy' 效果比 'categorical_crossentropy' 稍好。后者计算的loss直接=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbai/anaconda3/envs/huawei/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 37s 166ms/step - loss: 4638.3555 - precision: 0.1081 - acc: 0.1081 - val_loss: 1539.5272 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 11105.8760 - precision: 0.0916 - acc: 0.0916 - val_loss: 19857.4512 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 29912.4648 - precision: 0.0797 - acc: 0.0797 - val_loss: 1294.5199 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 9215.5479 - precision: 0.0828 - acc: 0.0828 - val_loss: 4769.1187 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 2778.1621 - precision: 0.0791 - acc: 0.0791 - val_loss: 1542.6415 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 5817.1089 - precision: 0.0744 - acc: 0.0744 - val_loss: 3913.2439 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 1488.2139 - precision: 0.0781 - acc: 0.0781 - val_loss: 27389.5371 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 7269.0732 - precision: 0.0916 - acc: 0.0916 - val_loss: 17175.9551 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 19617.8438 - precision: 0.1063 - acc: 0.1063 - val_loss: 231.6556 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 3738.6001 - precision: 0.1181 - acc: 0.1181 - val_loss: 6439.8789 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 22476.2129 - precision: 0.1241 - acc: 0.1241 - val_loss: 111.8147 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 2984.7515 - precision: 0.1322 - acc: 0.1322 - val_loss: 769.0327 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 1309.1422 - precision: 0.1284 - acc: 0.1284 - val_loss: 1515.1699 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 223.1065 - precision: 0.1359 - acc: 0.1359 - val_loss: 2307.4146 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 40s 201ms/step - loss: 4908.1528 - precision: 0.1273 - acc: 0.1273 - val_loss: 146.2496 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 40s 199ms/step - loss: 318.2555 - precision: 0.0969 - acc: 0.0969 - val_loss: 229.4521 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 115.6768 - precision: 0.0825 - acc: 0.0825 - val_loss: 160.2911 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 318.5273 - precision: 0.0825 - acc: 0.0825 - val_loss: 95.7032 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 34s 169ms/step - loss: 951.0951 - precision: 0.0772 - acc: 0.0772 - val_loss: 86.8637 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 32s 157ms/step - loss: 150.1412 - precision: 0.0794 - acc: 0.0794 - val_loss: 49.5646 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 1894.4567 - precision: 0.0759 - acc: 0.0759 - val_loss: 13.6742 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 31s 157ms/step - loss: 86.6623 - precision: 0.0862 - acc: 0.0862 - val_loss: 8.0185 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 348.2805 - precision: 0.0984 - acc: 0.0984 - val_loss: 347.0352 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 1515.4126 - precision: 0.1141 - acc: 0.1141 - val_loss: 301.6069 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 2087.8665 - precision: 0.1206 - acc: 0.1206 - val_loss: 2461.7200 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 903.3350 - precision: 0.1297 - acc: 0.1297 - val_loss: 1151.7284 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 752.0444 - precision: 0.1275 - acc: 0.1275 - val_loss: 33.9071 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 109.1585 - precision: 0.1359 - acc: 0.1359 - val_loss: 13.4695 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 67.8771 - precision: 0.1373 - acc: 0.1373 - val_loss: 280.9257 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 51.6368 - precision: 0.1034 - acc: 0.1034 - val_loss: 70.4906 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 6.5113 - precision: 0.0909 - acc: 0.0909 - val_loss: 44.3516 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 6.0733 - precision: 0.0778 - acc: 0.0778 - val_loss: 46.9203 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 28.2234 - precision: 0.0838 - acc: 0.0838 - val_loss: 16.5586 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 6.1445 - precision: 0.0766 - acc: 0.0766 - val_loss: 15.6964 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 12.6564 - precision: 0.0750 - acc: 0.0750 - val_loss: 3.9283 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 15.1143 - precision: 0.0822 - acc: 0.0822 - val_loss: 9.9292 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 272.8275 - precision: 0.0934 - acc: 0.0934 - val_loss: 62.0541 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 518.7280 - precision: 0.1081 - acc: 0.1081 - val_loss: 281.6760 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 643.6078 - precision: 0.1187 - acc: 0.1187 - val_loss: 10.6738 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 31s 157ms/step - loss: 267.1479 - precision: 0.1256 - acc: 0.1256 - val_loss: 1.2403 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 406.9994 - precision: 0.1322 - acc: 0.1322 - val_loss: 5.4387 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 29.3765 - precision: 0.1281 - acc: 0.1281 - val_loss: 2.2824 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 1.9049 - precision: 0.1428 - acc: 0.1428 - val_loss: 1.5698 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 49.6675 - precision: 0.1182 - acc: 0.1182 - val_loss: 2.8612 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 4.8480 - precision: 0.0919 - acc: 0.0919 - val_loss: 2.6417 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 31s 157ms/step - loss: 1.7527 - precision: 0.0816 - acc: 0.0816 - val_loss: 2.4002 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 3.9411 - precision: 0.0803 - acc: 0.0803 - val_loss: 1.9745 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 31s 157ms/step - loss: 2.2501 - precision: 0.0800 - acc: 0.0800 - val_loss: 1.4567 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 1.4112 - precision: 0.0791 - acc: 0.0791 - val_loss: 0.9398 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 1.0444 - precision: 0.0750 - acc: 0.0750 - val_loss: 0.8699 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 31s 156ms/step - loss: 158.5044 - precision: 0.0872 - acc: 0.0872 - val_loss: 0.9820 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 32s 158ms/step - loss: 75.8158 - precision: 0.1034 - acc: 0.1034 - val_loss: 0.7787 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 106.6949 - precision: 0.1163 - acc: 0.1163 - val_loss: 0.7548 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 199.1329 - precision: 0.1225 - acc: 0.1225 - val_loss: 0.7230 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 401.3863 - precision: 0.1303 - acc: 0.1303 - val_loss: 0.7135 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 1.0245 - precision: 0.1287 - acc: 0.1287 - val_loss: 0.7087 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.5230 - precision: 0.1356 - acc: 0.1356 - val_loss: 0.6946 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 4.6210 - precision: 0.1316 - acc: 0.1316 - val_loss: 0.7087 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 5.1974 - precision: 0.1031 - acc: 0.1031 - val_loss: 0.7490 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.4541 - precision: 0.0838 - acc: 0.0838 - val_loss: 0.7805 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 1.4329 - precision: 0.0825 - acc: 0.0825 - val_loss: 0.8020 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 1.9405 - precision: 0.0809 - acc: 0.0809 - val_loss: 0.8029 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.5288 - precision: 0.0775 - acc: 0.0775 - val_loss: 0.8079 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.8137 - precision: 0.0725 - acc: 0.0725 - val_loss: 0.8260 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.5572 - precision: 0.0841 - acc: 0.0841 - val_loss: 0.8018 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 32s 162ms/step - loss: 0.3946 - precision: 0.0956 - acc: 0.0956 - val_loss: 0.7735 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.5272 - precision: 0.1122 - acc: 0.1122 - val_loss: 0.7453 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 0.3942 - precision: 0.1184 - acc: 0.1184 - val_loss: 0.7278 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.4243 - precision: 0.1312 - acc: 0.1312 - val_loss: 0.7069 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 32s 161ms/step - loss: 12.0617 - precision: 0.1275 - acc: 0.1275 - val_loss: 0.7115 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 35s 174ms/step - loss: 3.7981 - precision: 0.1316 - acc: 0.1316 - val_loss: 0.7058 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.5317 - precision: 0.1412 - acc: 0.1412 - val_loss: 0.7011 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 3.9748 - precision: 0.1107 - acc: 0.1107 - val_loss: 0.7196 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.3666 - precision: 0.0928 - acc: 0.0928 - val_loss: 0.7636 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 35s 173ms/step - loss: 0.3173 - precision: 0.0806 - acc: 0.0806 - val_loss: 0.7916 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 34s 169ms/step - loss: 0.4350 - precision: 0.0812 - acc: 0.0812 - val_loss: 0.8006 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.5831 - precision: 0.0772 - acc: 0.0772 - val_loss: 0.8111 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 34s 168ms/step - loss: 0.3032 - precision: 0.0763 - acc: 0.0763 - val_loss: 0.8191 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 32s 159ms/step - loss: 0.3046 - precision: 0.0800 - acc: 0.0800 - val_loss: 0.8139 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 35s 173ms/step - loss: 0.3906 - precision: 0.0897 - acc: 0.0897 - val_loss: 0.7848 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 32s 160ms/step - loss: 0.9699 - precision: 0.1047 - acc: 0.1047 - val_loss: 0.7464 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.4588 - precision: 0.1178 - acc: 0.1178 - val_loss: 0.7246 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.4403 - precision: 0.1247 - acc: 0.1247 - val_loss: 0.7078 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.8431 - precision: 0.1322 - acc: 0.1322 - val_loss: 0.6965 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 34s 169ms/step - loss: 1.5854 - precision: 0.1284 - acc: 0.1284 - val_loss: 0.6986 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 2.4033 - precision: 0.1350 - acc: 0.1350 - val_loss: 0.6925 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 2.8821 - precision: 0.1279 - acc: 0.1279 - val_loss: 0.7051 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.5029 - precision: 0.0991 - acc: 0.0991 - val_loss: 0.7436 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 0.2928 - precision: 0.0816 - acc: 0.0816 - val_loss: 0.7797 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 0.2861 - precision: 0.0825 - acc: 0.0825 - val_loss: 0.7850 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 33s 163ms/step - loss: 0.2758 - precision: 0.0781 - acc: 0.0781 - val_loss: 0.7996 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2788 - precision: 0.0797 - acc: 0.0797 - val_loss: 0.7978 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.2676 - precision: 0.0756 - acc: 0.0756 - val_loss: 0.8053 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 35s 173ms/step - loss: 0.2914 - precision: 0.0856 - acc: 0.0856 - val_loss: 0.7843 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 34s 170ms/step - loss: 0.3229 - precision: 0.0981 - acc: 0.0981 - val_loss: 0.7545 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.3539 - precision: 0.1141 - acc: 0.1141 - val_loss: 0.7220 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 33s 165ms/step - loss: 0.3620 - precision: 0.1175 - acc: 0.1175 - val_loss: 0.7134 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.5224 - precision: 0.1306 - acc: 0.1306 - val_loss: 0.7040 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 33s 164ms/step - loss: 0.3862 - precision: 0.1284 - acc: 0.1284 - val_loss: 0.7005 - val_precision: 0.2917 - val_acc: 0.2917\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 35s 174ms/step - loss: 0.4018 - precision: 0.1372 - acc: 0.1372 - val_loss: 0.6823 - val_precision: 0.2917 - val_acc: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5334203bb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "    # Write TensorBoard logs to `./logs` directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./log/{}'.format(\n",
    "        dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M ResNet\")), write_images=True),\n",
    "    ]\n",
    "res_net_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=[tf.keras.metrics.Precision(),\n",
    "                               # tf.keras.metrics.PrecisionAtRecall(recall=0.1),\n",
    "                               \"acc\",\n",
    "                      ]\n",
    "                     )\n",
    "res_net_model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.target.shape: (17758, 672, 9, 1)\n",
      "Predicted: [[1.000 1.000 1.000 1.000 1.000 ... 1.000 1.000 1.000 1.000 1.000]] (1, 17758)\n",
      "True:       [0.000 0.000 0.000 0.000 0.000 ... 0.000 1.000 0.000 0.000 1.000] (17758,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "# X = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "# print(X.shape)\n",
    "y_pred = res_net_model.predict(X_test.target).transpose()\n",
    "print(\"Predicted:\", y_pred, y_pred.shape)\n",
    "print(\"True:      \", y_test.target, y_test.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}