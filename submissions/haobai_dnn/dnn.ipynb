{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Deep Neural Network\n",
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Test data\n"
     ]
    }
   ],
   "source": [
    "from others import load_all_dataset, rename_dataset\n",
    "X_train, y_train, X_test, y_test = load_all_dataset(show=False)\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=5,\n",
    "                    linewidth=1000,\n",
    "                    formatter={\"float\":lambda x: \"{:.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TRAIN SET ====\n",
      "Expected True: True\n",
      "  | X_source: (46110, 6048) ; y_source: (46110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-cd0deba31c2a>:36: RuntimeWarning: Mean of empty slice\n",
      "  nanmean.append(np.nanmean(x, axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected True: True\n",
      "A | X_source_bkg: (50862, 6048)\n",
      "Expected True: True\n",
      "----\n",
      "  | X_target: (438, 6048) ; y_target: (438,)\n",
      "Expected True: True\n",
      "B | X_target_bkg: (29592, 6048)\n",
      "Expected True: True\n",
      "  | X_target_unlabeled: (8202, 6048)\n",
      "==== TEST SET ====\n",
      "Expected True: True\n",
      "  | X_test.target: (17758, 6048) ; y_test.target: (17758,)\n",
      "Expected True: True\n",
      "B | X_test.target_bkg: (47275, 6048)\n",
      "  | X_test.target_unlabeled: None\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 去除NaN\n",
    "class FeatureExtractor:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        ''' Replace NaN by 0 and flatten the matrix to size (sample, 6720).\n",
    "        Executed on every input data (i.e., source, bkg, target) and passed\n",
    "        the resulting arrays to `fit`and `predict` methods in :class: Classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        `X`: ndarray of (sample, 672, 10)\n",
    "            3D input dataset(sample, time, features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        `X`: ndarray of (sample, 6720)\n",
    "            The filtered dataset\n",
    "        '''\n",
    "        #! ATTENTION\n",
    "        # The idea is supposed to eliminate the common columns filled entirely \n",
    "        # by NaN. But in this competition, since we don't have access to\n",
    "        # `OpticalDataset` object, it's impossible to communicate informations\n",
    "        # between datasets. So, here it deletes columns that are found on public\n",
    "        # dataset.\n",
    "        X = np.delete(X, [3,], axis=2)\n",
    "        X = X.astype(np.float64)\n",
    "        \n",
    "        ## 1st round\n",
    "        X1, nanmean = [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            nanmean.append(np.nanmean(x, axis=0))\n",
    "\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                X1.append(x) # deal later\n",
    "                continue\n",
    "            \n",
    "            # Rows with full Nan\n",
    "            # Unachievable. Cause we don't have access to manipulate on labels\n",
    "            # row_is_nan = np.all(indice, axis=1)\n",
    "            # if (row_is_nan == True).any():\n",
    "            #     row = np.where(row_is_nan == True)[0]\n",
    "            #     if len(row) >= x.shape[0]/4: # drop sample, /2=85%+, /4=75%+\n",
    "            #         continue\n",
    "            \n",
    "            # Columns with partial NaN\n",
    "            part_is_nan = np.any(indice, axis=0)\n",
    "            if (part_is_nan == True).any():\n",
    "                col = np.where(part_is_nan == True)[0]\n",
    "                # part_nan[i] = col[0]\n",
    "                for c in col:\n",
    "                    this = x[:,c]\n",
    "                    finite = this[np.isfinite(this)]\n",
    "                    fill = np.repeat(finite, np.ceil(len(this)/len(finite)))[:len(this)]\n",
    "                    x[:,c] = np.where(np.isfinite(this), this, fill)\n",
    "            \n",
    "            # Construct new array\n",
    "            X1.append(x)\n",
    "        X1, nanmean = np.array(X1), np.array(nanmean)\n",
    "\n",
    "        ## 2nd round\n",
    "        candidate_mean = []\n",
    "        for i in range(nanmean.shape[1]):\n",
    "            col = nanmean[i]\n",
    "            finite = col[np.isfinite(col)]\n",
    "            candidate_mean.append(finite)\n",
    "\n",
    "        X2 = []\n",
    "        for i in range(X1.shape[0]):\n",
    "            x = X[i]\n",
    "            indice = ~np.isfinite(x)\n",
    "            # Columns with full Nan\n",
    "            col_is_nan = np.all(indice, axis=0)\n",
    "            if (col_is_nan == True).any():\n",
    "                col = np.where(col_is_nan == True)[0]\n",
    "                for c in col:\n",
    "                    value = np.random.choice(candidate_mean[c])\n",
    "                    x = np.nan_to_num(x, nan=value)\n",
    "            X2.append(x)\n",
    "        \n",
    "        X = np.array(X2)\n",
    "\n",
    "        ## Final\n",
    "        X = X.reshape(X.shape[0], -1) # Flatten\n",
    "        # print(\"Expected True:\", np.all(np.isfinite(X))) # expected True\n",
    "        return X\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "[X_source, X_source_bkg, X_target, X_target_unlabeled, X_target_bkg,\n",
    "    y_source, y_target, X_test] = rename_dataset(\n",
    "    fe, X_train, y_train, X_test, y_test, show_imbalance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笔记:\n",
    "- batch size 越大，同样多epoch下，acc 越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集转换为TensorFlow格式\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_source, y_source)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_target, y_target)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.target, y_test.target))\n",
    "\n",
    "# 额外操作\n",
    "# train_dataset = train_dataset.map( lambda x, y: (tf.image.random_flip_left_right(x), y) ) # array must be 3D\n",
    "train_dataset = train_dataset.repeat()\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((None, 6048), (None,)), types: (tf.float64, tf.float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络模型\n",
    "参考资料\n",
    "\n",
    "1. [Build your first Neural Network in TensorFlow 2](https://towardsdatascience.com/building-your-first-neural-network-in-tensorflow-2-tensorflow-for-hackers-part-i-e1e2f1dfe7a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "model = tf.keras.Sequential()\n",
    "model.add( layers.Flatten(input_shape=(6048,), name=\"Input_Layer\") )\n",
    "\n",
    "num_fully_connected_layers = 10\n",
    "for i in range(num_fully_connected_layers):\n",
    "    model.add( layers.Dense(256, activation=\"relu\", name=\"Layer{}\".format(i+1)) )\n",
    "\n",
    "model.add( layers.Dropout(0.5, name=\"Layer-1\") )\n",
    "model.add( layers.Dense(1, activation='sigmoid', name=\"Output_Layer\") )\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.Precision(),\n",
    "                       # tf.keras.metrics.PrecisionAtRecall(recall=0.1),\n",
    "                       \"acc\",\n",
    "                      ]\n",
    "             )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "- if 'softmax' in the last layer, output is 0 or 1\n",
    "    - 'categorical_crossentropy' returns NaN, 'binary_crossentropy' acc ~ 0.1\n",
    "- if `'sigmoid'` in the last layer, output is the probability of 1\n",
    "    - 'categorical_crossentropy' returns NaN, `'binary_crossentropy'` acc ~ 0.8 to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 4s 12ms/step - loss: 61574.6055 - precision: 0.1607 - acc: 0.7755 - val_loss: 2696.6340 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 10607.0527 - precision: 0.1201 - acc: 0.8031 - val_loss: 28215.2520 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 13843.8096 - precision: 0.1345 - acc: 0.8019 - val_loss: 1946.8374 - val_precision: 0.4688 - val_acc: 0.6771\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 25406.6191 - precision: 0.1590 - acc: 0.8320 - val_loss: 14772.8779 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 37452.4844 - precision: 0.2795 - acc: 0.8562 - val_loss: 17342.8047 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 111006.5703 - precision: 0.3188 - acc: 0.8408 - val_loss: 1001.1895 - val_precision: 0.4583 - val_acc: 0.6771\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 41226.4102 - precision: 0.2599 - acc: 0.8355 - val_loss: 3844.9368 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 26324.9199 - precision: 0.2795 - acc: 0.8500 - val_loss: 3226.3425 - val_precision: 0.2857 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 39307.8867 - precision: 0.2523 - acc: 0.8748 - val_loss: 203.6909 - val_precision: 0.5333 - val_acc: 0.7083\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 537.4021 - precision: 0.2410 - acc: 0.8948 - val_loss: 6872.5806 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 12053.8623 - precision: 0.2350 - acc: 0.8830 - val_loss: 9162.7334 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 42958.4297 - precision: 0.2935 - acc: 0.8630 - val_loss: 4199.0996 - val_precision: 0.4286 - val_acc: 0.6562\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 45879.9297 - precision: 0.2376 - acc: 0.7989 - val_loss: 1023.8479 - val_precision: 0.3011 - val_acc: 0.3125\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 14623.6162 - precision: 0.2568 - acc: 0.8041 - val_loss: 3247.4434 - val_precision: 0.4615 - val_acc: 0.6771\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 6031.5034 - precision: 0.3088 - acc: 0.8439 - val_loss: 3831.5869 - val_precision: 0.4286 - val_acc: 0.6562\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 2093.0586 - precision: 0.2864 - acc: 0.8763 - val_loss: 2134.0930 - val_precision: 0.4615 - val_acc: 0.6875\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1533.4504 - precision: 0.2607 - acc: 0.8955 - val_loss: 2696.2268 - val_precision: 0.4000 - val_acc: 0.6667\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 3167.8184 - precision: 0.2641 - acc: 0.8966 - val_loss: 12233.7236 - val_precision: 0.4706 - val_acc: 0.6875\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 26838.5000 - precision: 0.1976 - acc: 0.8289 - val_loss: 2887.1438 - val_precision: 0.4333 - val_acc: 0.6562\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 57909.3516 - precision: 0.3107 - acc: 0.8334 - val_loss: 2486.6267 - val_precision: 0.4545 - val_acc: 0.6771\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 45047.0312 - precision: 0.2834 - acc: 0.8364 - val_loss: 908.7177 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 12366.6504 - precision: 0.2556 - acc: 0.8154 - val_loss: 12031.3438 - val_precision: 1.0000 - val_acc: 0.7083\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 10287.2061 - precision: 0.2829 - acc: 0.8891 - val_loss: 1653.9193 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 12000.9297 - precision: 0.2604 - acc: 0.8894 - val_loss: 1825.9395 - val_precision: 0.5556 - val_acc: 0.7188\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 9701.0684 - precision: 0.2376 - acc: 0.8798 - val_loss: 1218.1174 - val_precision: 0.4800 - val_acc: 0.6875\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 3749.2175 - precision: 0.2848 - acc: 0.8719 - val_loss: 579.9473 - val_precision: 0.5385 - val_acc: 0.7083\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 25476.2441 - precision: 0.2489 - acc: 0.8267 - val_loss: 5795.0093 - val_precision: 0.4615 - val_acc: 0.6771\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 17918.9648 - precision: 0.2679 - acc: 0.8380 - val_loss: 757.6177 - val_precision: 0.0000e+00 - val_acc: 0.6771\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 37122.8164 - precision: 0.3236 - acc: 0.8493 - val_loss: 672.7808 - val_precision: 0.4400 - val_acc: 0.6667\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 1113.0649 - precision: 0.2852 - acc: 0.8722 - val_loss: 1237.9954 - val_precision: 0.4737 - val_acc: 0.6875\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4909.9541 - precision: 0.2106 - acc: 0.8637 - val_loss: 507.1801 - val_precision: 0.4800 - val_acc: 0.6875\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 356.5952 - precision: 0.2483 - acc: 0.8908 - val_loss: 1620.6434 - val_precision: 0.4737 - val_acc: 0.6875\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 11743.4551 - precision: 0.2686 - acc: 0.8800 - val_loss: 2820.8826 - val_precision: 0.3333 - val_acc: 0.6875\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 6883.7969 - precision: 0.3727 - acc: 0.8739 - val_loss: 425.7887 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 11558.0195 - precision: 0.3927 - acc: 0.8578 - val_loss: 406.9159 - val_precision: 0.5263 - val_acc: 0.7083\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 6961.6987 - precision: 0.3780 - acc: 0.8491 - val_loss: 587.5909 - val_precision: 0.4706 - val_acc: 0.6875\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 2329.4265 - precision: 0.3166 - acc: 0.8720 - val_loss: 433.5828 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 7547.3262 - precision: 0.2727 - acc: 0.8839 - val_loss: 803.7852 - val_precision: 0.4583 - val_acc: 0.6771\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 5365.4482 - precision: 0.2329 - acc: 0.8781 - val_loss: 375.1273 - val_precision: 0.5294 - val_acc: 0.7083\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4417.6895 - precision: 0.2194 - acc: 0.8636 - val_loss: 4753.1079 - val_precision: 0.4444 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 13450.0967 - precision: 0.1914 - acc: 0.8147 - val_loss: 5494.5918 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 10724.5273 - precision: 0.3275 - acc: 0.8562 - val_loss: 1213.6964 - val_precision: 0.7143 - val_acc: 0.7292\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 6820.5879 - precision: 0.2998 - acc: 0.8422 - val_loss: 241.9594 - val_precision: 0.4762 - val_acc: 0.6875\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 1545.3331 - precision: 0.3523 - acc: 0.8646 - val_loss: 206.3026 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 329.1295 - precision: 0.2802 - acc: 0.8805 - val_loss: 220.0084 - val_precision: 0.5556 - val_acc: 0.7188\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 442.0877 - precision: 0.2969 - acc: 0.8984 - val_loss: 182.8992 - val_precision: 0.4737 - val_acc: 0.6875\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 862.8228 - precision: 0.2821 - acc: 0.8844 - val_loss: 319.5794 - val_precision: 0.4783 - val_acc: 0.6875\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 7297.4180 - precision: 0.2341 - acc: 0.8403 - val_loss: 385.9356 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 53986.9414 - precision: 0.2203 - acc: 0.7994 - val_loss: 971.3189 - val_precision: 0.4783 - val_acc: 0.6875\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 11084.6533 - precision: 0.3534 - acc: 0.8489 - val_loss: 962.0887 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 14851.8125 - precision: 0.2352 - acc: 0.8273 - val_loss: 715.0806 - val_precision: 0.5625 - val_acc: 0.7188\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 3309.7537 - precision: 0.1853 - acc: 0.8300 - val_loss: 1498.2831 - val_precision: 0.4000 - val_acc: 0.6771\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 35691.8867 - precision: 0.1818 - acc: 0.8559 - val_loss: 1085.5575 - val_precision: 0.6000 - val_acc: 0.7083\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 5165.2441 - precision: 0.1854 - acc: 0.8672 - val_loss: 1653.3698 - val_precision: 0.4706 - val_acc: 0.6875\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 65565.0234 - precision: 0.1908 - acc: 0.8391 - val_loss: 765.7562 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 11014.2422 - precision: 0.2428 - acc: 0.8141 - val_loss: 2139.6882 - val_precision: 0.4583 - val_acc: 0.6771\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 19948.3203 - precision: 0.2357 - acc: 0.8133 - val_loss: 6795.6914 - val_precision: 0.5714 - val_acc: 0.7083\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 20619.5371 - precision: 0.2181 - acc: 0.8043 - val_loss: 3683.9512 - val_precision: 0.5333 - val_acc: 0.7083\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 604.0708 - precision: 0.2305 - acc: 0.8641 - val_loss: 672.8838 - val_precision: 0.3333 - val_acc: 0.6771\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 7058.0273 - precision: 0.1686 - acc: 0.8395 - val_loss: 1537.5077 - val_precision: 0.5238 - val_acc: 0.7083\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4293.1890 - precision: 0.1778 - acc: 0.8506 - val_loss: 3008.4167 - val_precision: 0.4000 - val_acc: 0.6875\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 32070.6797 - precision: 0.1656 - acc: 0.8233 - val_loss: 4247.4282 - val_precision: 1.0000 - val_acc: 0.7083\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 34222.5273 - precision: 0.2116 - acc: 0.7962 - val_loss: 3156.0378 - val_precision: 0.4231 - val_acc: 0.6562\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 11958.1777 - precision: 0.2679 - acc: 0.8245 - val_loss: 993.7520 - val_precision: 0.6667 - val_acc: 0.7188\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 2920.8416 - precision: 0.3121 - acc: 0.8412 - val_loss: 65.0010 - val_precision: 0.7500 - val_acc: 0.7604\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 4882.2837 - precision: 0.2346 - acc: 0.8483 - val_loss: 1530.8251 - val_precision: 0.5238 - val_acc: 0.7083\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 1911.9915 - precision: 0.2046 - acc: 0.8552 - val_loss: 2664.8235 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 3836.9778 - precision: 0.2521 - acc: 0.8891 - val_loss: 639.0761 - val_precision: 0.5714 - val_acc: 0.7083\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 3217.2234 - precision: 0.2543 - acc: 0.8745 - val_loss: 803.1977 - val_precision: 0.8000 - val_acc: 0.7292\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4173.8667 - precision: 0.3073 - acc: 0.8605 - val_loss: 123.2420 - val_precision: 0.5714 - val_acc: 0.7188\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 2132.9067 - precision: 0.3235 - acc: 0.8408 - val_loss: 1339.6814 - val_precision: 0.5714 - val_acc: 0.7083\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 12009.6953 - precision: 0.3045 - acc: 0.8442 - val_loss: 138.0152 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1741.9883 - precision: 0.3134 - acc: 0.8743 - val_loss: 573.7253 - val_precision: 0.4348 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 2156.4978 - precision: 0.2885 - acc: 0.8908 - val_loss: 1795.5057 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1698.3389 - precision: 0.2424 - acc: 0.8800 - val_loss: 2911.7087 - val_precision: 0.0000e+00 - val_acc: 0.6771\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 404.6053 - precision: 0.2819 - acc: 0.9003 - val_loss: 29.3124 - val_precision: 0.5000 - val_acc: 0.6979\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 1.8160 - precision: 0.2982 - acc: 0.8863 - val_loss: 40.1415 - val_precision: 0.4000 - val_acc: 0.6875\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4.7145 - precision: 0.5263 - acc: 0.8731 - val_loss: 24.5182 - val_precision: 0.0000e+00 - val_acc: 0.6667\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.0494 - precision: 0.5273 - acc: 0.8675 - val_loss: 1.2664 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 13.1286 - precision: 0.4138 - acc: 0.8851 - val_loss: 2.5313 - val_precision: 0.0000e+00 - val_acc: 0.6771\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 4.1398 - precision: 0.4000 - acc: 0.9173 - val_loss: 1.3729 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3230 - precision: 0.5882 - acc: 0.9209 - val_loss: 1.3156 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4971 - precision: 0.5758 - acc: 0.9206 - val_loss: 1.2126 - val_precision: 0.0000e+00 - val_acc: 0.6771\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.4088 - precision: 0.3333 - acc: 0.8933 - val_loss: 1.0900 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.5543 - precision: 0.3226 - acc: 0.8741 - val_loss: 1.0948 - val_precision: 0.0000e+00 - val_acc: 0.6875\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4980 - precision: 0.4138 - acc: 0.8667 - val_loss: 0.9052 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.4467 - precision: 0.6071 - acc: 0.8796 - val_loss: 0.8918 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3171 - precision: 0.3103 - acc: 0.9141 - val_loss: 0.8791 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3327 - precision: 0.4000 - acc: 0.9173 - val_loss: 0.8580 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3288 - precision: 0.2000 - acc: 0.9233 - val_loss: 0.8797 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3819 - precision: 0.5833 - acc: 0.8995 - val_loss: 0.8603 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.8189 - precision: 0.2500 - acc: 0.8780 - val_loss: 0.8308 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4019 - precision: 0.7500 - acc: 0.8697 - val_loss: 0.8006 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 1.2785 - precision: 0.9000 - acc: 0.8695 - val_loss: 0.7779 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3119 - precision: 0.8333 - acc: 0.9116 - val_loss: 0.7818 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.2777 - precision: 0.5000 - acc: 0.9216 - val_loss: 0.7906 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2798 - precision: 0.1818 - acc: 0.9222 - val_loss: 0.8001 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3153 - precision: 0.0000e+00 - acc: 0.9058 - val_loss: 0.7967 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.3684 - precision: 0.6667 - acc: 0.8819 - val_loss: 0.7796 - val_precision: 0.0000e+00 - val_acc: 0.6979\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.4730 - precision: 0.6667 - acc: 0.8705 - val_loss: 0.7608 - val_precision: 0.0000e+00 - val_acc: 0.6979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5b744cec10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "callbacks = [\n",
    "    # Write TensorBoard logs to `./logs` directory\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./log/{}'.format(\n",
    "        dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M DNN\")), write_images=True),\n",
    "    ]\n",
    "model.fit(train_dataset, epochs=100, steps_per_epoch=200,\n",
    "          validation_data=valid_dataset, validation_steps=3,\n",
    "          # callbacks=callbacks\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.target.shape: (17758, 6048)\n",
      "Predicted: [[0.104 0.104 0.104 0.104 0.104 ... 0.104 0.104 0.104 0.104 0.104]] (1, 17758)\n",
      "True:       [0.000 0.000 0.000 0.000 0.000 ... 0.000 1.000 0.000 0.000 1.000] (17758,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.target.shape:\", X_test.target.shape)\n",
    "# X = X_test.target.reshape(X_test.target.shape[0], -1)\n",
    "# print(X.shape)\n",
    "y_pred = model.predict(X_test.target).transpose()\n",
    "print(\"Predicted:\", y_pred, y_pred.shape)\n",
    "print(\"True:      \", y_test.target, y_test.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77a176e6d68c62f570691917117cf1b3298ba06ea1b936eeef71e844f28195b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('huawei': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}