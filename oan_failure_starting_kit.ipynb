{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huawei Research France\n",
    "\n",
    "## Transfer learning on home network: \n",
    "\n",
    "## Build a transfer learning solution for home network failure prediction\n",
    "\n",
    "_Balázs Kégl, Aladin Virmaux, Illyyne Saffar, Jianfeng Zhang (Huawei Research, Noah's Ark Laboratory, France)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Optical access network is a main-stream Home Broadband Access Network solution around the world. It connects terminal subscribers to their service provider. When the network suffers a failure, an immense loss of\n",
    "data occurs, and affects hardly both of the quality of the service (QoS) and the user's experience (the quality of Experience QoE). To handle such issue and reduce the caused damage, it is important to predict in advance the network failures and fix them in time. Machine learning (ML) algoritms has been wildely used as a solution to build these failure prediction models. However, most of ML models are data-specific and are prone to degradation when the data distribution changes. This year's Huawei France hackathon aims to solve this problem. You will receive a labeled optical access network dataset from a city A (which we termed it as _source_ domain) and a mostly unlabeled  dataset (only 20% of labels) from a city B (which we termed it as _target_ domain). You are asked to build a transfer learning solution using the labaled source plus the unlabeled target to train a failure prediction model for city B. It is an **unsupervised domain adaptation** problem. An additional challenge can be faced from:\n",
    "\n",
    "1. **Missing values**: There are a lot of missing values in the data.\n",
    "2. **Time series sensor data**: Observation or samples are not mutually independent.\n",
    "3. **Class imbalance**: Network failure is a rare event, thus it is very imbalanced classification problem. \n",
    "4. **Scoring metrics**: Some of them are a nonstandard measures.\n",
    "\n",
    "\n",
    "## Context\n",
    "Transmission technologies have evolved to integrate optical technologies even in access networks, as close as possible to the subscriber. Due to its ability to propagate over long distances without signal regeneration, its low latency, and its very high bandwidth, fiber optic networks are so far the transmission medium par excellence. Optical fiber, initially deployed in very long distance and very high speed networks, is now tending to be generalized to offer more consuming services in terms of bandwidth. These are FTTH technologies for Fiber to the Home.\n",
    "\n",
    "The FTTH architecture generally adopted by operators is a PON (Passive Optical Network) architecture. The PON is a point-to-multipoint architecture based on the following elements:\n",
    "- A shared fiber optic infrastructure. The use of optical couplers in the network is the basis of the architecture and deployment engineering. The couplers are used to serve several zones or several subscribers.\n",
    "- A center equipment acting as Optical Line Termination (OLT). The OLT manages the broadcasting and reception of streams through network interfaces. It receives signals from subscribers and broadcasts a content based on specific services. \n",
    "- An end equipments.\n",
    "    - ONT (Optical Network Terminations) in case where the equipment is dedicated to a customer and the fiber reaches the customer. This is then an FTTH (Fiber To The Home) type architecture. There is only one fiber per customer (signals are bidirectional)\n",
    "    - ONU (optical network unit) in the case where the equipment is dedicated to a whole building. This is then an FTTB (Fiber To The Building).\n",
    "    \n",
    "<img src=\"https://image.makewebeasy.net/makeweb/0/p4Ky6EVg4/optical%20fiber-knowledge/Apps_FTTx_Fig3.png\">\n",
    "\n",
    "The data for this challenges is gathered from sensors collected in ONT level. \n",
    "\n",
    "### The data\n",
    "\n",
    "The data is coming from two different cities: city A (the source) and city B (the target). Data is labled for town A but unlabled for B (only 20% of labled data is known for city B). For both cities A and B data is a time serie gathered along a total of around 60 days. The time series granularity is 15 minutes. The samples reprensent different users (thus different ONT). At each time step, we have a 10 dimensional measurement of the following parameters (in parenthesis are the units of each feature).\n",
    "- data/ts/: \n",
    "  - current: bias current of the GPON ONT optical module (mA)\n",
    "  - err_down_bip: number of ONT downstream frames with BIP error (integer)\n",
    "  - err_up_bip: number of ONT upstream frames with BIP error (integer)\n",
    "  - olt_recv: receiving power of the GPON ONT optical module from the ONU (dBm)\n",
    "  - rdown: downstream rate of GPON ONT (Mbs)\n",
    "  - recv: receiving power of the GPON ONT optical module (dBm)\n",
    "  - rup: upstream rate of GPON ONT (Mbs)\n",
    "  - send: transmitting power of the GPON ONT optical module (dBm)\n",
    "  - temp: temperature of the GPON ONT optical module (Celsius)\n",
    "  - volt: power feed voltage of GPON ONT optical module (mV)\n",
    "- data/labels/: -1 (all good), 0 (weak) or 1 (failure) for sample. Let $x_t$ be the sample collected at the day $t$, then the label corresponding is computed on the day $t+7$. We aim to predict a failure from data coming 7 days before.  \n",
    "\n",
    "\n",
    "The data is given to you with shape **[users, timestamps, features]** and the features are given in the same orders as presented above.\n",
    "\n",
    "#### Missing data\n",
    "\n",
    "You will notice that a some data is missing in the datasets. There may be several reaons:\n",
    "\n",
    "1. no data was gathered on a specific date for a specific user.\n",
    "2. the data collecting process fail to retrieve a feature.\n",
    "    \n",
    "It is part of the challenge to overcome this real-life difficulty.\n",
    "\n",
    "### The scoring metrics\n",
    "\n",
    "In this challenge we propose to evaluate the performance using 5 different metrics:\n",
    "\n",
    "- **Acc** Accuracy: The number of truely predicted labels over the total number of the samples [sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score). \n",
    "- **Auc** roc_auc score: AUC measures the entire two-dimensional area underneath the ROC curve. This score gives us a good idea of how well the classifier will perform [sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).\n",
    "- **AP** Average precision: AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight: $\\sum_n (R_n - R_{n-1}) P_n$ where $P_n$ and $R_n$ are the precision and the recall at the $n$-th treshold [sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score).\n",
    "- **Precision@Recall**: It is a hybrid score implemented in `utils.scores` and it computes the precision when the recall is at some percentage, that is to say: \n",
    "  - rec@5: Recall at 5% computes the precision when the recall is at 5%.\n",
    "  - rec@10: Recall at 10% computes the precision when the recall is at 10%\n",
    "\n",
    "Note that the AP is the metric used for the final evaluation.\n",
    "\n",
    "\n",
    "## Competition rules\n",
    "\n",
    "The rules may be adjusted before the start of the challenge, June 16 2021.\n",
    "\n",
    "[//]: # \"* Submissions will be trained on a time series of roughly 50k samples with a  time window length of 672, across 10 features  and tested on a time series of roughly 35k samples.\" \n",
    "* The competition will end on June 27, 2020 at 18h UTC (20h in Paris).\n",
    "* The competitive phase will be followed by a collaborative phase when participants may (and encouraged to) reuse and combine each other's code.\n",
    "* The final results will be announced at the closing ceremony on June 30. We will also ask top teams to present their approaches at the event.\n",
    "* All models will be trained on the same cloud server allowing 76 CPUs (with shared memory of 500GB RAM).\n",
    "* Participants will be given a total of 20 machine hours (per cross-validation fold). Submissions of a given participant will be ordered by submission timestamp. We will make an attempt to train all submissions, but starting from (and including) the first submission that makes the participant's total training time exceed 20 hours, all submissions will be disqualified from the competition (but can enter into the collaborative phase). Testing time will not count towards the limit. Training time will be displayed on the leaderboard for all submissions, rounded to second. If a submission raises an exception, its training time will not count towards the total.\n",
    "* There is a timeout of 1 day between submissions that did not raise an exception.\n",
    "* Submissions submitted after the end of the competition will not qualify for prizes.\n",
    "* The public leaderboard will display validation scores running a cross-validation. The official scores will be calculated on the hidden test set and will be published after the closing of the competition. We will rank submissions according to their AP score.\n",
    "* The organizers will do their best so that the provided backend runs flawlessly. We will communicate with participants in case of concerns and will try to resolve all issues, but we reserve the right to make unilateral decisions in specific cases, not covered by this set of minimal rules.\n",
    "* The organizers reserve the right to disqualify any participant found to violate the fair competitive spirit of the challenge. Possible reasons, without being exhaustive, are multiple accounts, attempts to access the test data, etc.\n",
    "* Participants can form teams outside the platform before submitting any model individually, and submit on a single team account. Participating in more than one team at the same time is against the \"no multiple accounts\" rule, so, if discovered, may lead to disqualification. Before signing up, teams should communicate their composition and team name to BeMyApp.\n",
    "* Participants retain copyright on their submitted code and grant reuse under BSD 3-Clause License.\n",
    "\n",
    "Participants accept these rules automatically when making a submission at the RAMP site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the usual pydata libraries, you will need to install `ramp-workflow` from the advanced branch:\n",
    "```\n",
    "pip install git+https://github.com/paris-saclay-cds/ramp-workflow.git@advanced\n",
    "```\n",
    "\n",
    "\n",
    "It will install the `rampwf` library and the `ramp-test` script that you can use to check your submission before submitting. You do not need to know this package for participating in the challenge, but it could be useful to take a look at the [documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/advanced/index.html) if you would like to know what happens when we test your model, especially the [RAMP execution](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/advanced/scoring.html) page to understand `ramp-test`, and the [commands](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/advanced/command_line.html) to understand the different command line options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rampwf as rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `problem.py` so you can have an access to the same interface as the testing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = rw.utils.assert_read_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "First take the public data set from the .... #optical_network_challenge channel (join by [clicking here](link)) and unzip it to create `data/`. Note that the public data is different from the private one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data is composed of source and target data coming respectively from city A and city B. In real life FTTH problem presents 3 classes: 1) the case where the flow is normal and everything is going smoothly, 2) the case where the flow is poor but the conexion still working and 3) the case where we face a failure. Thus we can set 3 classes \\[good, poor, failure\\]. For the OAN failure detection we are interested in a binary classification between the two classes \\[poor, failure\\]. You are free to exploit the data of the \\[Good\\] class but in the scoring you are only judged on the binary classification. \n",
    "\n",
    "The dataset you are given is composed of:\n",
    "- The source data is composed of `X_train.source` and `X_train.source_bkg`. The labels of the source are accessible in `y_train.source`.\n",
    "    - `X_train.source`: Data for the classes \\[poor, failure\\]\n",
    "    - `X_train.source_bkg`: Data for the class \\[Good\\] \n",
    "    - `y_train.source`: Labels for the `X_train.source`, where 0: poor and 1 failure.\n",
    "- The target is composed of `X_train.target_labeled`, `X_train.target_unlabeled` and `X_train.target_bkg`. The labels of the target are accessible in `y_train.target`. \n",
    "    - `X_train.target_labeled`: Target labeled data for the classes \\[poor, failure\\]\n",
    "    - `X_train.target_unlabeled`: Target unlabeled data\n",
    "    - `X_train.target_bkg`: Target data for the class \\[Good\\] \n",
    "    - `y_train.target`: Labels for the `X_train.target_labeled`, where 0: poor and 1 failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Optical Dataset composed of\n",
      "46110 source samples\n",
      "50862 source background samples\n",
      "438 target labeled samples\n",
      "8202 target unlabeled samples\n",
      "29592 target background samples\n",
      " Optical Dataset labels composed of\n",
      "46110 labels of source samples\n",
      "438 labels of target samples\n",
      "\n",
      "Test data\n",
      "Optical Dataset composed of\n",
      "0 source samples\n",
      "0 source background samples\n",
      "17758 target labeled samples\n",
      "0 target unlabeled samples\n",
      "47275 target background samples\n",
      " Optical Dataset labels composed of\n",
      "0 labels of source samples\n",
      "17758 labels of target samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = problem.get_train_data()\n",
    "X_test, y_test = problem.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is a time series of 10 features and 672 time length. But, as you can see in the cells bellow, it contains nan values thus it should be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.30e+01, 0.00e+00, 0.00e+00, ..., 2.28e+00, 4.40e+01, 3.26e+03],\n",
       "       [1.30e+01, 0.00e+00, 0.00e+00, ..., 2.23e+00, 4.40e+01, 3.26e+03],\n",
       "       [1.30e+01, 0.00e+00, 0.00e+00, ..., 2.31e+00, 4.40e+01, 3.26e+03],\n",
       "       ...,\n",
       "       [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "       [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "       [     nan,      nan,      nan, ...,      nan,      nan,      nan]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.source[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4000e+01, 5.3560e+03, 0.0000e+00, ..., 2.1500e+00, 4.7000e+01,\n",
       "        3.3000e+03],\n",
       "       [1.4000e+01, 6.2650e+03, 2.0000e+00, ..., 2.0800e+00, 4.7000e+01,\n",
       "        3.3200e+03],\n",
       "       [1.4000e+01, 7.8850e+03, 4.0000e+00, ..., 2.4600e+00, 4.7000e+01,\n",
       "        3.3000e+03],\n",
       "       ...,\n",
       "       [1.4000e+01, 5.1556e+04, 6.0000e+00, ..., 2.3000e+00, 4.6000e+01,\n",
       "        3.3200e+03],\n",
       "       [1.4000e+01, 4.3742e+04, 2.0000e+01, ..., 1.9500e+00, 4.6000e+01,\n",
       "        3.3000e+03],\n",
       "       [1.4000e+01, 4.4794e+04, 2.6000e+01, ..., 2.2800e+00, 4.6000e+01,\n",
       "        3.3000e+03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.source[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classification task\n",
    "\n",
    "You can load here the naive domain adaptation implemented in the one of the folders named `starting_kit`. It is a naive transfer where the model trained on the source to classify the target using random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/classifier.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utils.dataset import OpticalDataset, OpticalLabels\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clf = RandomForestClassifier(\n",
    "            n_estimators=2, max_depth=2, random_state=44, n_jobs=-1)\n",
    "\n",
    "    def fit(self, X_source, X_source_bkg, X_target, X_target_unlabeled,\n",
    "            X_target_bkg, y_source, y_target):\n",
    "        self.clf.fit(X_source, y_source)\n",
    "\n",
    "    def predict_proba(self, X_target, X_target_bkg):\n",
    "        y_proba = self.clf.predict_proba(X_target)\n",
    "        return y_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train your submission and obtain test predictions using the same protocol as our training script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first you will need to flat the data to run the naive transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_workflow = problem.workflow.train_submission('submissions/starting_kit', X_train, y_train)\n",
    "y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scores\n",
    "\n",
    "We compute 5 scores on the classification. All scores are implemented in `external_imports.utils.scores.py` so you can look at the precise definitions there.\n",
    "**The official score of the competition is AP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP    = problem.score_types[0]\n",
    "rec5  = problem.score_types[1]\n",
    "rec10 = problem.score_types[2]\n",
    "rec20 = problem.score_types[3]\n",
    "acc   = problem.score_types[4]\n",
    "auc   = problem.score_types[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP test score    = 0.1626234044092243\n",
      "rec5 test score  = 0.07541412380122058\n",
      "rec10 test score = 0.1970357454228422\n",
      "rec20 test score = 0.34132519616390583\n",
      "acc test score   = 0.821939407590945\n",
      "auc test score   = 0.586923967966097\n"
     ]
    }
   ],
   "source": [
    "print('AP test score    = {}'.format(AP(y_test.target, y_test_pred[:,1])))\n",
    "print('rec5 test score  = {}'.format(rec5(y_test.target, y_test_pred[:,1])))\n",
    "print('rec10 test score = {}'.format(rec10(y_test.target, y_test_pred[:,1])))\n",
    "print('rec20 test score = {}'.format(rec20(y_test.target, y_test_pred[:,1])))\n",
    "print('acc test score   = {}'.format(acc(y_test.target, y_test_pred.argmax(axis=1))))\n",
    "print('auc test score   = {}'.format(auc(y_test.target, y_test_pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cross validation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation follows the same scheme as the train/test cut (see `problem.get_cv`): a 10 train/test cross validation folders across both the source and the target is  proposed for `problem.get_cv`. To learn more about these cuts details, you can check `external_imports.utils.cv.py` and then the `TLShuffleSplit` class.\n",
    "\n",
    "You are free to play with both the train/test cut and the cross-validation when developing your models but be aware that we will use the same set up on the official server as the one in the RAMP kit (on a different set of four campaigns that will not be available to you).\n",
    "\n",
    "The following cell goes through the same steps as the official evaluation script (`ramp-test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "training AP on fold 0 = 0.30833333333333335\n",
      "validation AP on fold 0 = 0.2637875964895809\n",
      "test AP on fold 0 = 0.16218430339780684\n",
      "-------------------------------------\n",
      "training AP on fold 1 = 0.21250000000000002\n",
      "validation AP on fold 1 = 0.2555942077788053\n",
      "test AP on fold 1 = 0.16361016472786805\n",
      "-------------------------------------\n",
      "training AP on fold 2 = 0.2\n",
      "validation AP on fold 2 = 0.29440601825201235\n",
      "test AP on fold 2 = 0.1745388926023523\n",
      "-------------------------------------\n",
      "training AP on fold 3 = 0.7375\n",
      "validation AP on fold 3 = 0.28218715512682335\n",
      "test AP on fold 3 = 0.16904411795376056\n",
      "-------------------------------------\n",
      "training AP on fold 4 = 0.21250000000000002\n",
      "validation AP on fold 4 = 0.24879604051634688\n",
      "test AP on fold 4 = 0.16172210972525408\n",
      "-------------------------------------\n",
      "training AP on fold 5 = 0.4\n",
      "validation AP on fold 5 = 0.30569665610952207\n",
      "test AP on fold 5 = 0.16752441466614315\n",
      "-------------------------------------\n",
      "training AP on fold 6 = 0.275\n",
      "validation AP on fold 6 = 0.2722666249472961\n",
      "test AP on fold 6 = 0.1703049715086814\n",
      "-------------------------------------\n",
      "training AP on fold 7 = 0.4\n",
      "validation AP on fold 7 = 0.28128936807137495\n",
      "test AP on fold 7 = 0.16824515986139207\n",
      "-------------------------------------\n",
      "training AP on fold 8 = 0.275\n",
      "validation AP on fold 8 = 0.2521365603657731\n",
      "test AP on fold 8 = 0.16847058901718204\n",
      "-------------------------------------\n",
      "training AP on fold 9 = 0.2\n",
      "validation AP on fold 9 = 0.25873354875375654\n",
      "test AP on fold 9 = 0.16532942916331855\n"
     ]
    }
   ],
   "source": [
    "splits = problem.get_cv(X_train, y_train)\n",
    "\n",
    "y_test_preds = []\n",
    "for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "    trained_workflow = problem.workflow.train_submission(\n",
    "        'submissions/starting_kit', X_train, y_train, train_is)\n",
    "    X_fold_train = X_train.slice(train_is)\n",
    "    X_fold_valid = X_train.slice(valid_is)\n",
    "    \n",
    "    y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "    y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "    y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "    print('-------------------------------------')\n",
    "    print('training AP on fold {} = {}'.format(fold_i, AP(y_train.slice(train_is).target, y_train_pred[:,1])))\n",
    "    print('validation AP on fold {} = {}'.format(fold_i, AP(y_train.slice(valid_is).target, y_valid_pred[:,1])))\n",
    "    print('test AP on fold {} = {}'.format(fold_i, AP(y_test.target, y_test_pred[:,1])))\n",
    "    \n",
    "    y_test_preds.append(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute both the mean test score and the score of bagging your eight models. The official ranking will be determined by the bagged test score (on different data sets from the ones you have). Your public score will be the bagged validation score (the averaging is [slightly more complicated](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/rampwf/utils/combine.py#L56) since we need to take care of the cross validation masks properly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AP score = 0.1670974152623759\n",
      "Bagged AP score = 0.16968309345277285\n"
     ]
    }
   ],
   "source": [
    "bagged_y_pred = np.array(y_test_preds).mean(axis=0)\n",
    "print('Mean AP score = {}'.format(\n",
    "    np.mean([AP(y_test.target, y_test_pred[:,1]) for y_test_pred in y_test_preds])))\n",
    "print('Bagged AP score = {}'.format(\n",
    "    AP(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example submissions\n",
    "\n",
    "Besides the starting kit implementing a naive transfer classifier using random forest, we provide you an other simple naive transfer example using xgboost, to get you started. \n",
    "\n",
    "Note that those submissions are not fully hyperparameters-optimized and do not come with every possible tricks. Feel free to play with it locally. There is no need to submit them since their respective score are already in the\n",
    "leaderboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning/ reshaping\n",
    "The data also needs to be cleaned proprely since it contains nan values. With every submission you find, besides of the `classifier.py`, a `feature_extractor.py`, where you can pre_process the data. For eg, in both `starting_kit` and `source_rf` or `target_rf` a data reshaping and a nan values handling are proposed in `FeatureExtractor.transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "    # Deal with NaNs inplace\n",
    "    np.nan_to_num(X, copy=False)\n",
    "    # We flatten the input, originally 3D (sample, time, dim) to\n",
    "    # 2D (sample, time * dim)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the `feature_extractor.py` to exploit or to preprocess more the data. Once again, do not hesitate to play with those examples locally to get familiar with RAMP and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source_rf submission\n",
    "This model is submitted under `submission.source_rf.classifier.py`. This model is very similar to the `starting_kit.classifier.py`. It differes with better the hyperparameters. \n",
    "\n",
    "In `source_rf` a random forest classifier is trained on the `X_train.source` data and tested on `X_test`. The oan failure is then detected without considering or exploiting the information in `X_train.source_bkg` or  `X_train.target_labeled`, `X_train.target_unlabeled`, `X_train.target_bkg`. This can be seen as the lower bound of the transfer learning for the oan failure detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/source_rf/classifier.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clf = RandomForestClassifier(\n",
    "            n_estimators=50, max_depth=10, random_state=44, n_jobs=-1)\n",
    "\n",
    "    def fit(self, X_source, X_source_bkg, X_target, X_target_unlabeled,\n",
    "            X_target_bkg, y_source, y_target):\n",
    "        self.clf.fit(X_source, y_source)\n",
    "\n",
    "    def predict_proba(self, X_target, X_target_bkg):\n",
    "        y_proba = self.clf.predict_proba(X_target)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target_rf submission\n",
    "\n",
    "This model is submitted under `submission.target_rf.classifier.py`.  In `target_rf` a random forest classifier is trained on the `X_train.target_labeled` data and tested on `X_test`. The oan failure is then detected without considering or exploiting the information in `X_train.source` or  `X_train.source_bkg`. This can be seen as the upper bound of the transfer learning for the oan failure detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/target_rf/classifier.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clf = RandomForestClassifier(\n",
    "            n_estimators=50, max_depth=10, random_state=44, n_jobs=-1)\n",
    "\n",
    "    def fit(self, X_source, X_source_bkg, X_target, X_target_unlabeled,\n",
    "            X_target_bkg, y_source, y_target):\n",
    "        self.clf.fit(X_target, y_target)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X_target, X_target_bkg):\n",
    "        y_proba = self.clf.predict_proba(X_target)\n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "|          | ap             | rec-5         | rec-10         | rec-20         | acc            |  auc           | \n",
    "|:---------|:--------------:|:-------------:|:--------------:|:--------------:|:--------------:|:--------------:|   \n",
    "|source_rf | 0.191 ± 0.0026 | 0.073 ± 0.002 | 0.176 ± 0.0032 | 0.357 ± 0.0075 | 0.84 ± 0.0014  | 0.637 ± 0.0063 | \n",
    "|target_rf | 0.163 ± 0.0218 | 0.067 ± 0.0182| 0.138 ± 0.0339 | 0.272 ± 0.0537 | 0.813 ± 0.036  | 0.591 ± 0.0399 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)\n",
    "\n",
    "You submission will contain a single `classifier.py` file implementing a classifier class with a `fit` and `predict_proba` function (scikit-learn API) as in the starting kit. You should place it in the `submission/<submission_name>` folder in your RAMP kit folder. To test your submission, go to your RAMP kit folder in a terminal and type\n",
    "```\n",
    "ramp-test --submission <submission_name>\n",
    "```\n",
    "It will train and test your submission much like we did it above in this notebook, and print the foldwise and summary scores. You can try it also in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ramp-test: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission target_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "1. First you will need to sign up at the [RAMP site](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com). Your team will be approved shortly by a system admin who will check that you communicated your team nick and composition to BeMyApp.\n",
    "2. You will then need a second sign-up, this time for the [optical network challenge](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton). If your site sign up was approved in the previous point, you should see a \"Join event\" button on the right of the top menu. This request will also be approved by a site admin.\n",
    "3. Once you are signed up, you can start submitting (once a day). If you are happy with your local scores, copy-paste your submission at the [sandbox](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton/sandbox), press \"submit now\", name your submission, then give credits to which other submission you used (in the collaborative phase you will see only your own submissions in the list.\n",
    "4. Your submission will be sent to train. It will either come back with an error or will be scored. You can follow the status at [my submissions](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton/my_submissions).\n",
    "5. If there is an error, click on the error to see the trace. You can resubmit a failed submission **under the same name**, this will not count in your daily quota.\n",
    "6. There is no way to delete trained submissions. In exceptional cases we can stop a submission that hasn't been scored yet so you can resubmit. We strongly suggest to finish training at least one fold locally (using `ramp-test`) before submitting so you can estimate the training time.\n",
    "7. You can follow the scores of the other participants at the [public leaderboard](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton/leaderboard).\n",
    "8. The public [competition leaderboard](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton/competition_leaderboard) displays the top submission (according to the public score) of each participant. You can change which of your submission enters the competition by pulling out the top submission. Click on the particular submission at [my submissions](https://ecs-90-84-188-77.compute.prod-cloud-ocb.orange-business.com/events/optical_network_modelling_hackaton/my_submissions) and click on the yellow button. The operation is reversible as many times you want, even after the competition deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "You can contact the organizers in the Slack of the challenge, join by [clicking here](link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CopperTL",
   "language": "python",
   "name": "coppertl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
